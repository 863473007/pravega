diff --git a/build.gradle b/build.gradle
index 803401c..4c9d45e 100644
--- a/build.gradle
+++ b/build.gradle
@@ -50,6 +50,7 @@ allprojects {
         compile group: 'commons-lang', name: 'commons-lang', version: '2.6'
         compile group: 'com.google.code.findbugs', name: 'jsr305', version: '1.3.9'
         compile group: 'org.projectlombok', name: 'lombok', version: '1.16.10'
+        compile group: 'commons-io', name: 'commons-io', version: '2.5'
         compile 'io.netty:netty-all:4.0.36.Final'
 
         testCompile "org.mockito:mockito-core:2.+"
@@ -227,6 +228,11 @@ project('controller:server') {
         compile project(":controller:contract")
         compile project(":clients:streaming")
         compile group: 'com.typesafe', name: 'config', version: '1.3.0'
+        compile group: 'org.apache.curator', name: 'curator-framework', version: '2.11.0'
+        compile group: 'org.apache.curator', name: 'curator-recipes', version: '2.11.0'
+        compile group: 'org.apache.curator', name: 'curator-client', version: '2.11.0'
+        compile group: 'org.apache.commons', name: 'commons-lang3', version: '3.3.2'
+        testCompile group: 'org.apache.curator', name: 'curator-test', version: '2.11.0'
     }
 }
 
diff --git a/clients/streaming/src/main/java/com/emc/pravega/stream/ScalingPolicy.java b/clients/streaming/src/main/java/com/emc/pravega/stream/ScalingPolicy.java
index c451464..5087ee1 100644
--- a/clients/streaming/src/main/java/com/emc/pravega/stream/ScalingPolicy.java
+++ b/clients/streaming/src/main/java/com/emc/pravega/stream/ScalingPolicy.java
@@ -19,11 +19,13 @@ package com.emc.pravega.stream;
 
 import lombok.Data;
 
+import java.io.Serializable;
+
 /**
  * A policy that specifies how the number of segments in a stream should scale over time.
  */
 @Data
-public class ScalingPolicy {
+public class ScalingPolicy implements Serializable {
     public enum Type {
         /**
          * No scaling, there will only ever be {@link ScalingPolicy#minNumSegments} at any given time.
diff --git a/clients/streaming/src/main/java/com/emc/pravega/stream/StreamConfiguration.java b/clients/streaming/src/main/java/com/emc/pravega/stream/StreamConfiguration.java
index 6571dcc..0879a7b 100644
--- a/clients/streaming/src/main/java/com/emc/pravega/stream/StreamConfiguration.java
+++ b/clients/streaming/src/main/java/com/emc/pravega/stream/StreamConfiguration.java
@@ -17,10 +17,12 @@
  */
 package com.emc.pravega.stream;
 
+import java.io.Serializable;
+
 /**
  * The configuration of a Stream 
  */
-public interface StreamConfiguration {
+public interface StreamConfiguration extends Serializable {
     
     /**
      * @return The scope of the stream
diff --git a/common/src/main/java/com/emc/pravega/common/concurrent/FutureCollectionHelper.java b/common/src/main/java/com/emc/pravega/common/concurrent/FutureCollectionHelper.java
new file mode 100644
index 0000000..065e64d
--- /dev/null
+++ b/common/src/main/java/com/emc/pravega/common/concurrent/FutureCollectionHelper.java
@@ -0,0 +1,84 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.common.concurrent;
+
+import com.google.common.base.Preconditions;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+import java.util.stream.Collectors;
+
+/**
+ * Utility methods for handling future collections.
+ */
+public class FutureCollectionHelper {
+
+    /**
+     * Predicate that evaluates in future.
+     * @param <T> Type parameter.
+     */
+    public interface FuturePredicate<T> {
+        CompletableFuture<Boolean> apply(T t);
+    }
+
+    /**
+     * Filter that takes a predicate that evaluates in future and returns the filtered list that evaluates in future.
+     * @param input Input list.
+     * @param predicate Predicate that evaluates in the future.
+     * @param <T> Type parameter.
+     * @return List that evaluates in future.
+     */
+    public static <T> CompletableFuture<List<T>> filter(List<T> input, FuturePredicate<T> predicate) {
+        Preconditions.checkNotNull(input);
+
+        List<CompletableFuture<Boolean>> future = input.stream().map(predicate::apply).collect(Collectors.toList());
+        return sequence(future).thenApply(
+                booleanList -> {
+                    List<T> result = new ArrayList<>();
+                    int i = 0;
+                    for (T elem : input) {
+                        if (booleanList.get(i)) {
+                            result.add(elem);
+                        }
+                        i++;
+                    }
+                    return result;
+                });
+    }
+
+    /**
+     * Converts a list of futures into a future list.
+     * If any of the futures in the input list completes exceptionally then the result completes exceptionally.
+     * Note that although this function uses a blocking join method, the function is not blocking, because join
+     * is invoked on a completed future.
+     * @param futures List of futures.
+     * @param <T> Type parameter.
+     * @return A future list.
+     */
+    public static <T> CompletableFuture<List<T>> sequence(List<CompletableFuture<T>> futures) {
+        CompletableFuture<Void> allDoneFuture =
+                CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()]));
+        return allDoneFuture.thenApply(v ->
+                        futures.stream()
+                                .map(CompletableFuture::join)
+                                .collect(Collectors.<T>toList())
+        );
+    }
+
+}
diff --git a/common/src/test/java/com/emc/pravega/common/concurrent/FutureCollectionHelperTest.java b/common/src/test/java/com/emc/pravega/common/concurrent/FutureCollectionHelperTest.java
new file mode 100644
index 0000000..bcb9d3f
--- /dev/null
+++ b/common/src/test/java/com/emc/pravega/common/concurrent/FutureCollectionHelperTest.java
@@ -0,0 +1,123 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.common.concurrent;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.CompletionException;
+import java.util.concurrent.ExecutionException;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+
+import static com.emc.pravega.common.concurrent.FutureCollectionHelper.*;
+
+/**
+ * Test methods for FutureCollectionHelpers
+ */
+public class FutureCollectionHelperTest {
+
+    /**
+     * Test method for FutureCollectionHelpers.filter
+     * @throws InterruptedException
+     * @throws ExecutionException
+     */
+    @Test
+    public void testFilter() throws ExecutionException, InterruptedException {
+        List<Integer> list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);
+
+        Predicate<Integer> evenFilter = (Integer x) -> x % 2 == 0;
+        FuturePredicate<Integer> futureEvenFilter = (Integer x) -> CompletableFuture.completedFuture(x % 2 == 0);
+
+        CompletableFuture<List<Integer>> filteredList =
+                filter(list, futureEvenFilter);
+
+        Assert.assertEquals(filteredList.get().size(), 3);
+        Assert.assertEquals(filteredList.get(), list.stream().filter(evenFilter).collect(Collectors.toList()));
+    }
+
+    /**
+     * Test method for FutureCollectionHelpers.filter when the FuturePredicate completes exceptionally in future
+     */
+    @Test(expected = CompletionException.class)
+    public void testFilterException() {
+        List<Integer> list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);
+
+        FuturePredicate<Integer> futureEvenFilter =
+                (Integer x) -> CompletableFuture.supplyAsync(() -> {
+                    throw new RuntimeException();
+                });
+
+        CompletableFuture<List<Integer>> filteredList =
+                filter(list, futureEvenFilter);
+
+        filteredList.join();
+    }
+
+    /**
+     * Test method for FutureCollectionHelpers.sequence
+     * @throws ExecutionException
+     * @throws InterruptedException
+     */
+    @Test
+    public void testSequence() throws ExecutionException, InterruptedException {
+        List<CompletableFuture<Integer>> list = new ArrayList<>();
+
+        int n = 10;
+        for (int i = 0; i < n; i++) {
+            CompletableFuture<Integer> x = new CompletableFuture<>();
+            x.complete(i);
+            list.add(x);
+        }
+
+        CompletableFuture<List<Integer>> sequence = sequence(list);
+
+        List<Integer> returnList = sequence.get();
+        Assert.assertEquals(returnList.size(), n);
+        for (int i = 0; i < n; i++) {
+            Assert.assertEquals(returnList.get(i).intValue(), i);
+        }
+    }
+
+    /**
+     * Test method for FutureCollectionHelpers.sequence when some elements in original list complete exceptionally in future
+     */
+    @Test(expected = CompletionException.class)
+    public void testSequenceException() {
+        List<CompletableFuture<Integer>> list = new ArrayList<>();
+
+        int n = 10;
+        for (int i = 0; i < n; i++) {
+            CompletableFuture<Integer> x = new CompletableFuture<>();
+            if (i % 2 == 1) {
+                x.completeExceptionally(new RuntimeException());
+            } else {
+                x.complete(i);
+            }
+            list.add(x);
+        }
+
+        CompletableFuture<List<Integer>> sequence = sequence(list);
+
+        sequence.join();
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/server/Main.java b/controller/server/src/main/java/com/emc/pravega/controller/server/Main.java
index a21404a..d954dc6 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/server/Main.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/server/Main.java
@@ -1,66 +1,67 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package com.emc.pravega.controller.server;
-
-import static com.emc.pravega.controller.util.Config.HOST_STORE_TYPE;
-import static com.emc.pravega.controller.util.Config.STREAM_STORE_TYPE;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-
-import com.emc.pravega.controller.server.rpc.RPCServer;
-import com.emc.pravega.controller.server.rpc.v1.ControllerServiceImpl;
-import com.emc.pravega.controller.store.host.Host;
-import com.emc.pravega.controller.store.host.HostControllerStore;
-import com.emc.pravega.controller.store.host.HostStoreFactory;
-import com.emc.pravega.controller.store.host.InMemoryHostControllerStoreConfig;
-import com.emc.pravega.controller.store.stream.StreamMetadataStore;
-import com.emc.pravega.controller.store.stream.StreamStoreFactory;
-import com.google.common.collect.Sets;
-
-import lombok.extern.slf4j.Slf4j;
-
-/**
- * Entry point of controller server.
- */
-@Slf4j
-public class Main {
-
-    public static void main(String[] args) {
-
-        // TODO: Will use hard-coded host to container mapping for this sprint
-        // Read from a config file. This same information will be present on pravega hosts
-        // TODO: remove temporary hard coding for the cluster and segment
-        Map<Host, Set<Integer>> hostContainerMap = new HashMap<>();
-        hostContainerMap.put(new Host("localhost", 12345), Sets.newHashSet(0));
-
-        //1) LOAD configuration.
-        log.info("Creating in-memory stream store");
-        StreamMetadataStore streamStore = StreamStoreFactory.createStore(
-                StreamStoreFactory.StoreType.valueOf(STREAM_STORE_TYPE), null);
-        log.info("Creating in-memory host store");
-        HostControllerStore hostStore = HostStoreFactory.createStore(HostStoreFactory.StoreType.valueOf(HOST_STORE_TYPE),
-                new InMemoryHostControllerStoreConfig(hostContainerMap));
-
-        //2) start the Server implementations.
-        //2.1) start RPC server with v1 implementation. Enable other versions if required.
-        log.info("Starting RPC server");
-        RPCServer.start(new ControllerServiceImpl(streamStore, hostStore));
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.server;
+
+import static com.emc.pravega.controller.util.Config.HOST_STORE_TYPE;
+import static com.emc.pravega.controller.util.Config.STREAM_STORE_CONNECTION_STRING;
+import static com.emc.pravega.controller.util.Config.STREAM_STORE_TYPE;
+
+import com.emc.pravega.controller.server.rpc.RPCServer;
+import com.emc.pravega.controller.server.rpc.v1.ControllerServiceAsyncImpl;
+import com.emc.pravega.controller.store.host.Host;
+import com.emc.pravega.controller.store.host.HostControllerStore;
+import com.emc.pravega.controller.store.host.HostStoreFactory;
+import com.emc.pravega.controller.store.host.InMemoryHostControllerStoreConfig;
+import com.emc.pravega.controller.store.stream.StoreConfiguration;
+import com.emc.pravega.controller.store.stream.StreamMetadataStore;
+import com.emc.pravega.controller.store.stream.StreamStoreFactory;
+import lombok.extern.slf4j.Slf4j;
+import com.google.common.collect.Sets;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * Entry point of controller server.
+ */
+@Slf4j
+public class Main {
+
+    public static void main(String[] args) {
+
+        // TODO: Will use hard-coded host to container mapping for this sprint
+        // Read from a config file. This same information will be present on pravega hosts
+        // TODO: remove temporary hard coding for the cluster and segment
+        Map<Host, Set<Integer>> hostContainerMap = new HashMap<>();
+        hostContainerMap.put(new Host("localhost", 12345), Sets.newHashSet(0));
+
+        //1) LOAD configuration.
+        log.info("Creating in-memory stream store");
+        StreamMetadataStore streamStore = StreamStoreFactory.createStore(
+                StreamStoreFactory.StoreType.valueOf(STREAM_STORE_TYPE),
+                new StoreConfiguration(STREAM_STORE_CONNECTION_STRING));
+        log.info("Creating in-memory host store");
+        HostControllerStore hostStore = HostStoreFactory.createStore(HostStoreFactory.StoreType.valueOf(HOST_STORE_TYPE),
+                new InMemoryHostControllerStoreConfig(hostContainerMap));
+
+        //2) start RPC server with v1 implementation. Enable other versions if required.
+        log.info("Starting RPC server");
+        RPCServer.start(new ControllerServiceAsyncImpl(streamStore, hostStore));
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/RPCServer.java b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/RPCServer.java
index 46bc5c5..ff2a71a 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/RPCServer.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/RPCServer.java
@@ -43,12 +43,10 @@ import static com.emc.pravega.controller.util.Config.SERVER_WORKER_THREAD_COUNT;
 @Slf4j
 public class RPCServer {
 
-    public static void start(ControllerService.Iface controllerService) {
+    public static void start(ControllerService.AsyncIface controllerService) {
         try {
 
-            Runnable simple = () -> {
-                threadedSelectorServer(new ControllerService.Processor(controllerService));
-            };
+            Runnable simple = () -> threadedSelectorServer(new ControllerService.AsyncProcessor<>(controllerService));
 
             new Thread(simple).start();
         } catch (Exception x) {
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceAsyncImpl.java b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceAsyncImpl.java
new file mode 100644
index 0000000..e5c635a
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceAsyncImpl.java
@@ -0,0 +1,114 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.server.rpc.v1;
+
+import com.emc.pravega.controller.store.host.HostControllerStore;
+import com.emc.pravega.controller.store.stream.StreamMetadataStore;
+import com.emc.pravega.controller.stream.api.v1.ControllerService;
+import com.emc.pravega.controller.stream.api.v1.Position;
+import com.emc.pravega.controller.stream.api.v1.SegmentId;
+import com.emc.pravega.controller.stream.api.v1.StreamConfig;
+import com.emc.pravega.controller.stream.api.v1.TxId;
+import com.emc.pravega.stream.impl.model.ModelHelper;
+import lombok.extern.slf4j.Slf4j;
+import org.apache.thrift.TException;
+import org.apache.thrift.async.AsyncMethodCallback;
+
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+
+/**
+ * Asynchronous controller service implementation
+ */
+@Slf4j
+public class ControllerServiceAsyncImpl implements ControllerService.AsyncIface {
+
+    private final ControllerServiceImpl controllerService;
+
+    public ControllerServiceAsyncImpl(StreamMetadataStore streamStore, HostControllerStore hostStore) {
+        controllerService = new ControllerServiceImpl(streamStore, hostStore);
+    }
+
+    @Override
+    public void createStream(StreamConfig streamConfig, AsyncMethodCallback resultHandler) throws TException {
+        log.debug("createStream called for stream " + streamConfig.getScope() + "/" + streamConfig.getName());
+        processResult(controllerService.createStream(ModelHelper.encode(streamConfig)), resultHandler);
+    }
+
+    @Override
+    public void alterStream(StreamConfig streamConfig, AsyncMethodCallback resultHandler) throws TException {
+        log.debug("alterStream called for stream " + streamConfig.getScope() + "/" + streamConfig.getName());
+        processResult(controllerService.alterStream(ModelHelper.encode(streamConfig)), resultHandler);
+    }
+
+    @Override
+    public void getCurrentSegments(String scope, String stream, AsyncMethodCallback resultHandler) throws TException {
+        log.debug("getCurrentSegments called for stream " + scope + "/" + stream);
+        processResult(controllerService.getCurrentSegments(scope, stream), resultHandler);
+    }
+
+    @Override
+    public void getPositions(String scope, String stream, long timestamp, int count, AsyncMethodCallback resultHandler) throws TException {
+        log.debug("getPositions called for stream " + scope + "/" + stream);
+        processResult(controllerService.getPositions(scope, stream, timestamp, count), resultHandler);
+    }
+
+    @Override
+    public void updatePositions(String scope, String stream, List<Position> positions, AsyncMethodCallback resultHandler) throws TException {
+        log.debug("updatePositions called for stream " + scope + "/" + stream);
+        processResult(controllerService.updatePositions(scope, stream, positions), resultHandler);
+    }
+
+    @Override
+    public void getURI(SegmentId segment, AsyncMethodCallback resultHandler) throws TException {
+        log.debug("getURI called for segment " + segment.getScope() + "/" + segment.getStreamName() + "/" + segment.getNumber());
+        processResult(controllerService.getURI(segment), resultHandler);
+    }
+
+    @Override
+    public void createTransaction(String scope, String stream, AsyncMethodCallback resultHandler) throws TException {
+
+    }
+
+    @Override
+    public void commitTransaction(String scope, String stream, TxId txid, AsyncMethodCallback resultHandler) throws TException {
+
+    }
+
+    @Override
+    public void dropTransaction(String scope, String stream, TxId txid, AsyncMethodCallback resultHandler) throws TException {
+
+    }
+
+    @Override
+    public void checkTransactionStatus(String scope, String stream, TxId txid, AsyncMethodCallback resultHandler) throws TException {
+
+    }
+
+    private static <T> void processResult(CompletableFuture<T> result, AsyncMethodCallback resultHandler) {
+        result.whenComplete(
+                (value, ex) -> {
+                    log.debug("result = " + value.toString());
+                    if (ex != null) {
+                        resultHandler.onError(new RuntimeException(ex));
+                    } else {
+                        resultHandler.onComplete(value);
+                    }
+                });
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceImpl.java b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceImpl.java
index b5fa731..98b5126 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceImpl.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceImpl.java
@@ -25,23 +25,20 @@ import java.util.Set;
 import java.util.concurrent.CompletableFuture;
 import java.util.stream.Collectors;
 
+import com.emc.pravega.stream.Segment;
+import com.emc.pravega.stream.StreamConfiguration;
 import org.apache.commons.lang.NotImplementedException;
 import org.apache.thrift.TException;
 
 import com.emc.pravega.controller.store.host.HostControllerStore;
-import com.emc.pravega.controller.store.stream.Segment;
 import com.emc.pravega.controller.store.stream.SegmentFutures;
 import com.emc.pravega.controller.store.stream.StreamMetadataStore;
-import com.emc.pravega.controller.stream.api.v1.ControllerService;
 import com.emc.pravega.controller.stream.api.v1.FutureSegment;
 import com.emc.pravega.controller.stream.api.v1.NodeUri;
 import com.emc.pravega.controller.stream.api.v1.Position;
 import com.emc.pravega.controller.stream.api.v1.SegmentId;
 import com.emc.pravega.controller.stream.api.v1.SegmentRange;
 import com.emc.pravega.controller.stream.api.v1.Status;
-import com.emc.pravega.controller.stream.api.v1.StreamConfig;
-import com.emc.pravega.controller.stream.api.v1.TxId;
-import com.emc.pravega.controller.stream.api.v1.TxStatus;
 import com.emc.pravega.stream.PositionInternal;
 import com.emc.pravega.stream.impl.model.ModelHelper;
 import com.emc.pravega.stream.impl.netty.ConnectionFactoryImpl;
@@ -52,7 +49,7 @@ import com.google.common.collect.Multimaps;
 /**
  * Stream controller RPC server implementation
  */
-public class ControllerServiceImpl implements ControllerService.Iface {
+public class ControllerServiceImpl {
 
     private final StreamMetadataStore streamStore;
     private final HostControllerStore hostStore;
@@ -64,63 +61,86 @@ public class ControllerServiceImpl implements ControllerService.Iface {
         this.connectionFactory = new ConnectionFactoryImpl(false);
     }
 
-    /**
-     * Create the stream metadata in the metadata streamStore.
-     * Start with creation of minimum number of segments.
-     * Asynchronously call createSegment on pravega hosts about segments in the stream
-     */
-    @Override
-    public Status createStream(StreamConfig streamConfig) throws TException {
+    public CompletableFuture<Status> createStream(StreamConfiguration streamConfig) {
         String stream = streamConfig.getName();
 
-        if (streamStore.createStream(stream, ModelHelper.encode(streamConfig))) {
-            streamStore.getActiveSegments(stream)
-                .getCurrent()
-                .stream()
-                .parallel()
-                .forEach(i -> notifyNewSegment(streamConfig.getScope(), stream, i));
-            return Status.SUCCESS;
-        } else {
-            return Status.FAILURE;
-        }
-    }
-    
-    public void notifyNewSegment(String scope, String stream, int segmentNumber) {
-        NodeUri uri = SegmentHelper.getSegmentUri(scope, stream, segmentNumber, hostStore);
-
-        // async call, dont wait for its completion or success. Host will contact controller if it does not know
-        // about some segment even if this call fails
-        CompletableFuture.runAsync(() -> SegmentHelper.createSegment(scope, stream, segmentNumber, ModelHelper.encode(uri), connectionFactory));
+        return streamStore.createStream(stream, streamConfig, System.currentTimeMillis())
+                .thenApply(result -> {
+                    if (result) {
+                        streamStore.getActiveSegments(stream)
+                                .thenApply(activeSegments -> {
+                                    activeSegments
+                                            .stream()
+                                            .parallel()
+                                            .forEach(segment -> notifyNewSegment(streamConfig.getScope(), stream, segment.getNumber()));
+                                    return null;
+                                });
+                        return Status.SUCCESS;
+                    } else {
+                        return Status.FAILURE;
+                    }
+                });
     }
 
-    @Override
-    public Status alterStream(StreamConfig streamConfig) throws TException {
+    public CompletableFuture<Status> alterStream(StreamConfiguration streamConfig) {
         throw new NotImplementedException();
     }
 
-    @Override
-    public List<SegmentRange> getCurrentSegments(String scope, String stream) throws TException {        
+    public CompletableFuture<List<SegmentRange>> getCurrentSegments(String scope, String stream) {
         // fetch active segments from segment store
-        SegmentFutures activeSegments = streamStore.getActiveSegments(stream);
-        List<SegmentRange> segments = activeSegments.getCurrent().stream().map(number -> {
-            Segment segment = streamStore.getSegment(stream, number);
-            return new SegmentRange(new SegmentId(scope, stream, number), segment.getKeyStart(), segment.getKeyEnd());
-        }).collect(Collectors.toList());
-        return segments;    
+        return streamStore.getActiveSegments(stream)
+                .thenApply(activeSegments -> activeSegments
+                                .stream()
+                                .map(segment ->
+                                                new SegmentRange(
+                                                        new SegmentId(scope, stream, segment.getNumber()),
+                                                        segment.getKeyStart(),
+                                                        segment.getKeyEnd())
+                                )
+                                .collect(Collectors.toList())
+                );
     }
 
-    @Override
-    public NodeUri getURI(SegmentId segment) throws TException {
-        return SegmentHelper.getSegmentUri(segment.getScope(), segment.getStreamName(), segment.getNumber(), hostStore);
+    public CompletableFuture<List<Position>> getPositions(String scope, String stream, long timestamp, int count) {
+        // first fetch segments active at specified timestamp from the specified stream
+        // divide current segments in segmentFutures into at most count positions
+        return streamStore.getActiveSegments(stream, timestamp)
+                .thenApply(segmentFutures -> shard(scope, stream, segmentFutures, count));
     }
 
-    @Override
-    public List<Position> getPositions(String scope, String stream, long timestamp, int count) throws TException {
-        // first fetch segments active at specified timestamp from the specified stream
-        SegmentFutures segmentFutures = streamStore.getActiveSegments(stream, timestamp);
+    public CompletableFuture<List<Position>> updatePositions(String scope, String stream, List<Position> positions) {
+        // TODO: handle npe with null exception return case
+        List<PositionInternal> internalPositions = positions.stream().map(ModelHelper::encode).collect(Collectors.toList());
+        // initialize completed segments set from those found in the list of input position objects
+        Set<Integer> completedSegments = internalPositions.stream().flatMap(position ->
+                        position.getCompletedSegments().stream().map(Segment::getSegmentNumber)
+        ).collect(Collectors.toSet());
+
+        Map<Integer, Long> segmentOffsets = new HashMap<>();
+
+        // convert positions to segmentFutures, while updating completedSegments set and
+        // storing segment offsets in segmentOffsets map
+        List<SegmentFutures> segmentFutures = convertPositionsToSegmentFutures(internalPositions, segmentOffsets);
+
+        // fetch updated SegmentFutures from stream metadata
+        // and finally convert SegmentFutures back to position objects
+        return streamStore.getNextSegments(stream, completedSegments, segmentFutures)
+                .thenApply(updatedSegmentFutures ->
+                        convertSegmentFuturesToPositions(scope, stream, updatedSegmentFutures, segmentOffsets));
+    }
+
+    public CompletableFuture<NodeUri> getURI(SegmentId segment) throws TException {
+        return CompletableFuture.completedFuture(
+                SegmentHelper.getSegmentUri(segment.getScope(), segment.getStreamName(), segment.getNumber(), hostStore)
+        );
+    }
+
+    private void notifyNewSegment(String scope, String stream, int segmentNumber) {
+        NodeUri uri = SegmentHelper.getSegmentUri(scope, stream, segmentNumber, hostStore);
 
-        // divide current segments in segmentFutures into at most n positions
-        return shard(scope, stream, segmentFutures, timestamp, count);
+        // async call, dont wait for its completion or success. Host will contact controller if it does not know
+        // about some segment even if this call fails
+        CompletableFuture.runAsync(() -> SegmentHelper.createSegment(scope, stream, segmentNumber, ModelHelper.encode(uri), connectionFactory));
     }
 
     /**
@@ -134,7 +154,7 @@ public class ControllerServiceImpl implements ControllerService.Iface {
      * @param n number of shards
      * @return the list of position objects
      */
-    private List<Position> shard(String scope, String stream, SegmentFutures segmentFutures, long timestamp, int n) {
+    private List<Position> shard(String scope, String stream, SegmentFutures segmentFutures, int n) {
         // divide the active segments equally into at most n partition
         int currentCount = segmentFutures.getCurrent().size();
         int quotient = currentCount / n;
@@ -187,40 +207,15 @@ public class ControllerServiceImpl implements ControllerService.Iface {
         }
         return positions;
     }
-    
-    
-    @Override
-    public List<Position> updatePositions(String scope, String stream, List<Position> positions) throws TException {
-        // TODO: handle npe with null exception return case
-        List<PositionInternal> internalPositions = positions.stream().map(ModelHelper::encode).collect(Collectors.toList());
-        // initialize completed segments set from those found in the list of input position objects
-        Set<Integer> completedSegments = internalPositions.stream().flatMap(position -> 
-            position.getCompletedSegments().stream().map(segment -> segment.getSegmentNumber())
-        ).collect(Collectors.toSet());
-        
-        Map<Integer, Long> segmentOffsets = new HashMap<>();
-
-        // convert positions to segmentFutures, while updating completedSegments set and
-        // storing segment offsets in segmentOffsets map
-        List<SegmentFutures> segmentFutures = convertPositionsToSegmentFutures(stream, internalPositions, completedSegments, segmentOffsets);
-
-        // fetch updated SegmentFutures from stream metadata
-        List<SegmentFutures> updatedSegmentFutures = streamStore.getNextSegments(stream, completedSegments, segmentFutures);
 
-        // finally convert SegmentFutures back to position objects
-        return convertSegmentFuturesToPositions(scope, stream, updatedSegmentFutures, segmentOffsets);
-    }
-    
     /**
      * This method converts list of positions into list of segmentFutures.
      * While doing so it updates the completedSegments set and stores segment offsets in a map.
-     * @param stream input stream
      * @param positions input list of positions
-     * @param completedSegments set of completed segments that shall be updated in this method
      * @param segmentOffsets map of segment number of its offset that shall be populated in this method
      * @return the list of segmentFutures objects
      */
-    private List<SegmentFutures> convertPositionsToSegmentFutures(String stream, List<PositionInternal> positions, Set<Integer> completedSegments, Map<Integer, Long> segmentOffsets) {
+    private List<SegmentFutures> convertPositionsToSegmentFutures(List<PositionInternal> positions, Map<Integer, Long> segmentOffsets) {
         List<SegmentFutures> segmentFutures = new ArrayList<>(positions.size());
 
         // construct SegmentFutures for each position object.
@@ -231,9 +226,6 @@ public class ControllerServiceImpl implements ControllerService.Iface {
                     x -> {
                         int number = x.getKey().getSegmentNumber();
                         current.add(number);
-                        Segment segment = streamStore.getSegment(stream, number);
-                        // update completed segments set with implicitly completed segments
-                        segment.getPredecessors().stream().forEach(y -> completedSegments.add(y));
                         segmentOffsets.put(number, x.getValue());
                     }
             );
@@ -260,29 +252,4 @@ public class ControllerServiceImpl implements ControllerService.Iface {
         );
         return resultPositions;
     }
-
-    @Override
-    public TxId createTransaction(String scope, String stream) throws TException {
-        // TODO Auto-generated method stub
-        return null;
-    }
-
-    @Override
-    public Status commitTransaction(String scope, String stream, TxId txid) throws TException {
-        // TODO Auto-generated method stub
-        return null;
-    }
-
-    @Override
-    public Status dropTransaction(String scope, String stream, TxId txid) throws TException {
-        // TODO Auto-generated method stub
-        return null;
-    }
-
-    @Override
-    public TxStatus checkTransactionStatus(String scope, String stream, TxId txid) throws TException {
-        // TODO Auto-generated method stub
-        return null;
-    }
-    
 }
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceSyncImpl.java b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceSyncImpl.java
new file mode 100644
index 0000000..1631b74
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/server/rpc/v1/ControllerServiceSyncImpl.java
@@ -0,0 +1,109 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.server.rpc.v1;
+
+import com.emc.pravega.common.concurrent.FutureHelpers;
+import com.emc.pravega.controller.store.host.HostControllerStore;
+import com.emc.pravega.controller.store.stream.StreamMetadataStore;
+import com.emc.pravega.controller.stream.api.v1.ControllerService;
+import com.emc.pravega.controller.stream.api.v1.NodeUri;
+import com.emc.pravega.controller.stream.api.v1.Position;
+import com.emc.pravega.controller.stream.api.v1.SegmentId;
+import com.emc.pravega.controller.stream.api.v1.SegmentRange;
+import com.emc.pravega.controller.stream.api.v1.Status;
+import com.emc.pravega.controller.stream.api.v1.StreamConfig;
+import com.emc.pravega.controller.stream.api.v1.TxId;
+import com.emc.pravega.controller.stream.api.v1.TxStatus;
+import com.emc.pravega.stream.impl.model.ModelHelper;
+import org.apache.commons.lang.NotImplementedException;
+import org.apache.thrift.TException;
+
+import java.util.List;
+
+/**
+ * Synchronous controller service implementation
+ */
+public class ControllerServiceSyncImpl implements ControllerService.Iface {
+
+    private final ControllerServiceImpl controllerService;
+
+    public ControllerServiceSyncImpl(StreamMetadataStore streamStore, HostControllerStore hostStore) {
+        controllerService = new ControllerServiceImpl(streamStore, hostStore);
+    }
+
+    /**
+     * Create the stream metadata in the metadata streamStore.
+     * Start with creation of minimum number of segments.
+     * Asynchronously call createSegment on pravega hosts notifying them about new segments in the stream.
+     */
+    @Override
+    public Status createStream(StreamConfig streamConfig) throws TException {
+        return FutureHelpers.getAndHandleExceptions(controllerService.createStream(ModelHelper.encode(streamConfig)), RuntimeException::new);
+    }
+
+    @Override
+    public Status alterStream(StreamConfig streamConfig) throws TException {
+        throw new NotImplementedException();
+    }
+
+    @Override
+    public List<SegmentRange> getCurrentSegments(String scope, String stream) throws TException {
+        return FutureHelpers.getAndHandleExceptions(controllerService.getCurrentSegments(scope, stream), RuntimeException::new);
+    }
+
+    @Override
+    public NodeUri getURI(SegmentId segment) throws TException {
+        return FutureHelpers.getAndHandleExceptions(controllerService.getURI(segment), RuntimeException::new);
+    }
+
+    @Override
+    public List<Position> getPositions(String scope, String stream, long timestamp, int count) throws TException {
+        return FutureHelpers.getAndHandleExceptions(controllerService.getPositions(scope, stream, timestamp, count), RuntimeException::new);
+    }
+
+    @Override
+    public List<Position> updatePositions(String scope, String stream, List<Position> positions) throws TException {
+        return FutureHelpers.getAndHandleExceptions(controllerService.updatePositions(scope, stream, positions), RuntimeException::new);
+    }
+
+    @Override
+    public TxId createTransaction(String scope, String stream) throws TException {
+        // TODO Auto-generated method stub
+        return null;
+    }
+
+    @Override
+    public Status commitTransaction(String scope, String stream, TxId txid) throws TException {
+        // TODO Auto-generated method stub
+        return null;
+    }
+
+    @Override
+    public Status dropTransaction(String scope, String stream, TxId txid) throws TException {
+        // TODO Auto-generated method stub
+        return null;
+    }
+
+    @Override
+    public TxStatus checkTransactionStatus(String scope, String stream, TxId txid) throws TException {
+        // TODO Auto-generated method stub
+        return null;
+    }
+
+
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/host/InMemoryHostControllerStoreConfig.java b/controller/server/src/main/java/com/emc/pravega/controller/store/host/InMemoryHostControllerStoreConfig.java
index 53e9fa5..f28b005 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/store/host/InMemoryHostControllerStoreConfig.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/host/InMemoryHostControllerStoreConfig.java
@@ -25,12 +25,13 @@ import java.util.Set;
 
 import static com.emc.pravega.controller.util.Config.HOST_STORE_CONTAINER_COUNT;
 
-public class InMemoryHostControllerStoreConfig implements StoreConfiguration {
+public class InMemoryHostControllerStoreConfig extends StoreConfiguration {
     private final int numOfContainers = HOST_STORE_CONTAINER_COUNT;
 
     private final Map<Host, Set<Integer>> hostContainerMap;
 
     public InMemoryHostControllerStoreConfig(Map<Host, Set<Integer>> hostContainerMap) {
+        super("");
         this.hostContainerMap = hostContainerMap;
     }
 
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/AbstractStreamMetadataStore.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/AbstractStreamMetadataStore.java
new file mode 100644
index 0000000..3a1da62
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/AbstractStreamMetadataStore.java
@@ -0,0 +1,291 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import com.emc.pravega.stream.StreamConfiguration;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Sets;
+
+import java.util.AbstractMap;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.CompletableFuture;
+import java.util.stream.Collectors;
+
+import static com.emc.pravega.common.concurrent.FutureCollectionHelper.filter;
+import static com.emc.pravega.common.concurrent.FutureCollectionHelper.sequence;
+
+
+/**
+ * Abstract Stream metadata store. It implements various read queries using the Stream interface.
+ * Implementation of create and update queries are delegated to the specific implementations of this abstract class.
+ */
+public abstract class AbstractStreamMetadataStore implements StreamMetadataStore {
+
+    abstract Stream getStream(String name);
+
+    @Override
+    public CompletableFuture<Boolean> updateConfiguration(String name, StreamConfiguration configuration) {
+        return getStream(name).updateConfiguration(configuration);
+    }
+
+    @Override
+    public CompletableFuture<StreamConfiguration> getConfiguration(String name) {
+        return getStream(name).getConfiguration();
+    }
+
+    @Override
+    public CompletableFuture<Segment> getSegment(String name, int number) {
+        return getStream(name).getSegment(number);
+    }
+
+    @Override
+    public CompletableFuture<List<Segment>> getActiveSegments(String name) {
+        Stream stream = getStream(name);
+        return stream
+                .getActiveSegments()
+                .thenCompose(currentSegments -> sequence(currentSegments.stream().map(stream::getSegment).collect(Collectors.toList())));
+    }
+
+    @Override
+    public CompletableFuture<SegmentFutures> getActiveSegments(String name, long timestamp) {
+        Stream stream = getStream(name);
+        CompletableFuture<List<Integer>> futureActiveSegments = stream.getActiveSegments(timestamp);
+        return futureActiveSegments.thenCompose(activeSegments -> constructSegmentFutures(stream, activeSegments));
+    }
+
+    @Override
+    public CompletableFuture<List<SegmentFutures>> getNextSegments(String name, Set<Integer> completedSegments, List<SegmentFutures> positions) {
+        Preconditions.checkNotNull(positions);
+        Preconditions.checkArgument(positions.size() > 0);
+
+        Stream stream = getStream(name);
+
+        Set<Integer> current = new HashSet<>();
+        positions.forEach(position ->
+                position.getCurrent().forEach(current::add));
+
+        CompletableFuture<Set<Integer>> implicitCompletedSegments =
+                getImplicitCompletedSegments(stream, completedSegments, current);
+
+        CompletableFuture<Set<Integer>> successors =
+                implicitCompletedSegments.thenCompose(x -> getSuccessors(stream, x));
+
+        CompletableFuture<List<Integer>> newCurrents =
+                implicitCompletedSegments.thenCompose(x -> successors.thenCompose(y -> getNewCurrents(stream, y, x, positions)));
+
+        CompletableFuture<Map<Integer, List<Integer>>> newFutures =
+                implicitCompletedSegments.thenCompose(x -> successors.thenCompose(y -> getNewFutures(stream, y, x)));
+
+        return newCurrents.thenCompose(x -> newFutures.thenCompose(y -> divideSegments(stream, x, y, positions)));
+    }
+
+    @Override
+    public CompletableFuture<List<Segment>> scale(String name, List<Integer> sealedSegments, List<AbstractMap.SimpleEntry<Double, Double>> newRanges, long scaleTimestamp) {
+        return getStream(name).scale(sealedSegments, newRanges, scaleTimestamp);
+    }
+
+    private CompletableFuture<SegmentFutures> constructSegmentFutures(Stream stream, List<Integer> activeSegments) {
+        Map<Integer, Integer> futureSegments = new HashMap<>();
+        List<CompletableFuture<List<Integer>>> list =
+                activeSegments.stream().map(number -> getDefaultFutures(stream, number)).collect(Collectors.toList());
+
+        CompletableFuture<List<List<Integer>>> futureDefaultFutures = sequence(list);
+        return futureDefaultFutures
+                .thenApply(futureList -> {
+                            for (int i = 0; i < futureList.size(); i++) {
+                                for (Integer future : futureList.get(i)) {
+                                    futureSegments.put(future, activeSegments.get(i));
+                                }
+                            }
+                            return new SegmentFutures(activeSegments, futureSegments);
+                        }
+                );
+    }
+
+    /**
+     * Finds all successors of a given segment, that have exactly one predecessor,
+     * and hence can be included in the futures of the given segment.
+     * @param stream input stream
+     * @param number segment number for which default futures are sought.
+     * @return the list of successors of specified segment who have only one predecessor.
+     *
+     *         return stream.getSuccessors(number).stream()
+     *               .filter(x -> stream.getPredecessors(x).size() == 1)
+    .*                collect(Collectors.toList());
+     */
+    private CompletableFuture<List<Integer>> getDefaultFutures(Stream stream, int number) {
+        CompletableFuture<List<Integer>> futureSuccessors = stream.getSuccessors(number);
+        return futureSuccessors.thenCompose(
+                list -> filter(list, elem -> stream.getPredecessors(elem).thenApply(x -> x.size() == 1)));
+    }
+
+    private CompletableFuture<Set<Integer>> getImplicitCompletedSegments(Stream stream, Set<Integer> completedSegments, Set<Integer> current) {
+        List<CompletableFuture<Set<Integer>>> futures =
+                current
+                        .stream()
+                        .map(x -> stream.getPredecessors(x).thenApply(list -> list.stream().collect(Collectors.toSet())))
+                        .collect(Collectors.toList());
+
+        return sequence(futures)
+                .thenApply(list -> Sets.union(completedSegments, list.stream().reduce(Collections.emptySet(), Sets::union)));
+        // append completed segment set with implicitly completed segments
+//        return foldFutures(
+//                sequence(futures),
+//                new HashSet<>(),
+//                (CompletableFuture<Set<Integer>> x, List<Integer> y) -> x.thenApply(set -> {
+//                    set.addAll(y);
+//                    return set;
+//                }),
+//                Sets::union
+//        ).thenApply(set -> Sets.union(set, completedSegments));
+    }
+
+    private CompletableFuture<Set<Integer>> getSuccessors(Stream stream, Set<Integer> completedSegments) {
+        // successors of completed segments are interesting, which means
+        // some of them may become current, and
+        // some of them may become future
+        //Set<Integer> successors = completedSegments.stream().flatMap(x -> stream.getSuccessors(x).stream()).collect(Collectors.toSet());
+        List<CompletableFuture<Set<Integer>>> futures =
+                completedSegments
+                        .stream()
+                        .map(x -> stream.getSuccessors(x).thenApply(list -> list.stream().collect(Collectors.toSet())))
+                        .collect(Collectors.toList());
+
+        return sequence(futures)
+                .thenApply(list -> list.stream().reduce(Collections.emptySet(), Sets::union));
+//        return foldFutures(
+//                sequence(futures),
+//                new HashSet<>(),
+//                (CompletableFuture<Set<Integer>> x, List<Integer> y) -> x.thenApply(set -> {
+//                    set.addAll(y);
+//                    return set;
+//                }),
+//                Sets::union
+//        );
+    }
+
+    private CompletableFuture<List<Integer>> getNewCurrents(Stream stream, Set<Integer> successors, Set<Integer> completedSegments, List<SegmentFutures> positions) {
+        // a successor that has
+        // 1. it is not completed yet, and
+        // 2. it is not current in any of the positions, and
+        // 3. all its predecessors completed.
+        // shall become current and be added to some position
+        List<Integer> newCurrents = successors.stream().filter(x ->
+                        // 2. it is not completed yet, and
+                        !completedSegments.contains(x)
+                        // 3. it is not current in any of the positions
+                        && positions.stream().allMatch(z -> !z.getCurrent().contains(x))
+        ).collect(Collectors.toList());
+
+        // 3. all its predecessors completed, and
+        return filter(
+                newCurrents,
+                (Integer x) -> stream.getPredecessors(x).thenApply(list -> list.stream().allMatch(completedSegments::contains))
+        );
+    }
+
+    private CompletableFuture<Map<Integer, List<Integer>>> getNewFutures(Stream stream, Set<Integer> successors, Set<Integer> completedSegments) {
+        List<Integer> subset = successors.stream().filter(x -> !completedSegments.contains(x)).collect(Collectors.toList());
+
+        List<CompletableFuture<List<Integer>>> predecessors = new ArrayList<>();
+        for (Integer number: subset) {
+            predecessors.add(stream.getPredecessors(number)
+                            .thenApply(preds -> preds.stream().filter(y -> !completedSegments.contains(y)).collect(Collectors.toList()))
+            );
+        }
+
+        return sequence(predecessors).thenApply((List<List<Integer>> preds) -> {
+            Map<Integer, List<Integer>> map = new HashMap<>();
+            for (int i = 0; i < preds.size(); i++) {
+                List<Integer> filtered = preds.get(i);
+                if (filtered.size() == 1) {
+                    Integer pendingPredecessor = filtered.get(0);
+                    if (map.containsKey(pendingPredecessor)) {
+                        map.get(pendingPredecessor).add(subset.get(i));
+                    } else {
+                        List<Integer> list = new ArrayList<>();
+                        list.add(subset.get(i));
+                        map.put(pendingPredecessor, list);
+                    }
+                }
+            }
+            return map;
+        });
+    }
+
+    /**
+     * Divides the set of new current segments among existing positions and returns the updated positions
+     * @param stream stream
+     * @param newCurrents new set of current segments
+     * @param newFutures new set of future segments
+     * @param positions positions to be updated
+     * @return the updated sequence of positions
+     */
+    private CompletableFuture<List<SegmentFutures>> divideSegments(Stream stream, List<Integer> newCurrents, Map<Integer, List<Integer>> newFutures, List<SegmentFutures> positions) {
+        List<CompletableFuture<SegmentFutures>> newPositions = new ArrayList<>(positions.size());
+
+        int quotient = newCurrents.size() / positions.size();
+        int remainder = newCurrents.size() % positions.size();
+        int counter = 0;
+        for (int i = 0; i < positions.size(); i++) {
+            SegmentFutures position = positions.get(i);
+
+            // add the new current segments
+            List<Integer> newCurrent = new ArrayList<>(position.getCurrent());
+            int portion = (i < remainder) ? quotient + 1 : quotient;
+            for (int j = 0; j < portion; j++, counter++) {
+                newCurrent.add(newCurrents.get(counter));
+            }
+            Map<Integer, Integer> newFuture = new HashMap<>(position.getFutures());
+            // add new futures if any
+            position.getCurrent().forEach(
+                    current -> {
+                        if (newFutures.containsKey(current)) {
+                            newFutures.get(current).stream().forEach(x -> newFuture.put(x, current));
+                        }
+                    }
+            );
+            // add default futures for new and old current segments, if any
+            List<CompletableFuture<List<Integer>>> defaultFutures =
+                    newCurrent.stream().map(x -> getDefaultFutures(stream, x)).collect(Collectors.toList());
+
+            CompletableFuture<SegmentFutures> segmentFuture =
+                    sequence(defaultFutures).thenApply((List<List<Integer>> list) -> {
+                        for (int k = 0; k < list.size(); k++) {
+                            Integer x = newCurrent.get(k);
+                            list.get(k).stream().forEach(
+                                    y -> {
+                                        if (!newFuture.containsKey(y)) {
+                                            newFuture.put(y, x);
+                                        }
+                                    }
+                            );
+                        }
+                        return new SegmentFutures(newCurrent, newFuture);
+                    });
+            newPositions.add(segmentFuture);
+        }
+        return sequence(newPositions);
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemorySegment.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemorySegment.java
new file mode 100644
index 0000000..f3a6de4
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemorySegment.java
@@ -0,0 +1,64 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+/**
+ * In-memory representation of a stream segment.
+ */
+import com.google.common.base.Preconditions;
+import lombok.Data;
+import lombok.EqualsAndHashCode;
+import lombok.ToString;
+
+import java.util.ArrayList;
+import java.util.List;
+
+@Data
+@ToString(includeFieldNames = true)
+@EqualsAndHashCode(callSuper = true)
+public class InMemorySegment extends Segment {
+
+    enum Status {
+        Active,
+        Sealing,
+        Sealed,
+    }
+
+    private final long end;
+    private final Status status;
+    private final List<Integer> successors;
+    private final List<Integer> predecessors;
+
+    InMemorySegment(int number, long start, long end, double keyStart, double keyEnd) {
+        super(number, start, keyStart, keyEnd);
+        this.end = end;
+        this.status = Status.Active;
+        successors = new ArrayList<>();
+        predecessors = new ArrayList<>();
+    }
+
+    InMemorySegment(int number, long start, long end, double keyStart, double keyEnd, Status status, List<Integer> successors, List<Integer> predecessors) {
+        super(number, start, keyStart, keyEnd);
+        Preconditions.checkNotNull(successors);
+        Preconditions.checkNotNull(predecessors);
+        this.end = end;
+        this.status = status;
+        this.successors = successors;
+        this.predecessors = predecessors;
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStream.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStream.java
new file mode 100644
index 0000000..44f0c61
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStream.java
@@ -0,0 +1,184 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import com.emc.pravega.stream.StreamConfiguration;
+import com.google.common.base.Preconditions;
+
+import java.util.AbstractMap.SimpleEntry;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+import java.util.stream.IntStream;
+
+/**
+ * Stream properties
+ */
+class InMemoryStream implements Stream {
+    private final String name;
+    private StreamConfiguration configuration;
+
+    /**
+     * Stores all segments in the stream, ordered by number, which implies that
+     * these segments are also ordered in the increaing order of their start times.
+     * Segment number is the index of that segment in this list.
+     */
+    private final List<InMemorySegment> segments = new ArrayList<>();
+
+    /**
+     * Stores segment numbers of currently active segments in the stream.
+     * It enables efficient access to current segments needed by producers and tailing consumers.
+     */
+    private final List<Integer> currentSegments = new ArrayList<>();
+
+    InMemoryStream(String name) {
+        this.name = name;
+    }
+
+    @Override
+    public CompletableFuture<Boolean> create(StreamConfiguration configuration, long timeStamp) {
+        this.configuration = configuration;
+        int numSegments = configuration.getScalingingPolicy().getMinNumSegments();
+        double keyRange = 1.0 / numSegments;
+        IntStream.range(0, numSegments)
+                .forEach(
+                        x -> {
+                            InMemorySegment segment = new InMemorySegment(x, 0, Long.MAX_VALUE, x * keyRange, (x + 1) * keyRange);
+                            segments.add(segment);
+                            currentSegments.add(x);
+                        }
+                );
+        return CompletableFuture.completedFuture(true);
+    }
+
+    @Override
+    public String getName() {
+        return this.name;
+    }
+
+    @Override
+    public synchronized CompletableFuture<Boolean> updateConfiguration(StreamConfiguration configuration) {
+        this.configuration = configuration;
+        return CompletableFuture.completedFuture(true);
+    }
+
+    @Override
+    public synchronized CompletableFuture<StreamConfiguration> getConfiguration() {
+        return CompletableFuture.completedFuture(this.configuration);
+    }
+
+    @Override
+    public synchronized CompletableFuture<Segment> getSegment(int number) {
+        return CompletableFuture.completedFuture(segments.get(number));
+    }
+
+    @Override
+    public CompletableFuture<List<Integer>> getSuccessors(int number) {
+        return CompletableFuture.completedFuture(segments.get(number).getSuccessors());
+    }
+
+    @Override
+    public CompletableFuture<List<Integer>> getPredecessors(int number) {
+        return CompletableFuture.completedFuture(segments.get(number).getPredecessors());
+    }
+
+    /**
+     * @return the list of currently active segments
+     */
+    @Override
+    public synchronized CompletableFuture<List<Integer>> getActiveSegments() {
+        return CompletableFuture.completedFuture(Collections.unmodifiableList(currentSegments));
+    }
+
+    /**
+     * @return the list of segments active at a given timestamp.
+     * GetActiveSegments runs in O(n), where n is the total number of segments.
+     * It can be improved to O(k + logn), where k is the number of active segments at specified timestamp,
+     * using augmented interval tree or segment index..
+     * TODO: maintain a augmented interval tree or segment tree index
+     */
+    @Override
+    public synchronized CompletableFuture<List<Integer>> getActiveSegments(long timestamp) {
+        List<Integer> currentSegments = new ArrayList<>();
+        int i = 0;
+        while (i < segments.size() && timestamp >= segments.get(i).getStart()) {
+            if (segments.get(i).getEnd() >= timestamp) {
+                InMemorySegment segment = segments.get(i);
+                currentSegments.add(segment.getNumber());
+            }
+            i++;
+        }
+        return CompletableFuture.completedFuture(currentSegments);
+    }
+
+    /**
+     * Seals a set of segments, and adds a new set of segments as current segments.
+     * It sets appropriate endtime and successors of sealed segment.
+     * @param sealedSegments segments to be sealed
+     * @param keyRanges    new segments to be added as active segments
+     * @param scaleTimestamp scaling timestamp. This will be the end time of sealed segments and start time of new segments.
+     * @return the list of new segments.
+     */
+    @Override
+    public synchronized CompletableFuture<List<Segment>> scale(List<Integer> sealedSegments, List<SimpleEntry<Double, Double>> keyRanges, long scaleTimestamp) {
+        Preconditions.checkNotNull(sealedSegments);
+        Preconditions.checkNotNull(keyRanges);
+        Preconditions.checkArgument(sealedSegments.size() > 0);
+        Preconditions.checkArgument(keyRanges.size() > 0);
+
+        List<List<Integer>> predecessors = new ArrayList<>();
+        for (int i = 0; i < keyRanges.size(); i++) {
+            predecessors.add(new ArrayList<>());
+        }
+
+        int start = segments.size();
+        // assign status, end times, and successors to sealed segments.
+        // assign predecessors to new segments
+        for (Integer sealed: sealedSegments) {
+            InMemorySegment segment = segments.get(sealed);
+            List<Integer> successors = new ArrayList<>();
+
+            for (int i = 0; i < keyRanges.size(); i++) {
+                if (segment.overlaps(keyRanges.get(i).getKey(), keyRanges.get(i).getValue())) {
+                    successors.add(start + i);
+                    predecessors.get(i).add(sealed);
+                }
+            }
+            InMemorySegment sealedSegment = new InMemorySegment(sealed, segment.getStart(), scaleTimestamp, segment.getKeyStart(), segment.getKeyEnd(), InMemorySegment.Status.Sealed, successors, segment.getPredecessors());
+            segments.set(sealed, sealedSegment);
+            currentSegments.remove(sealed);
+        }
+
+        List<Segment> newSegments = new ArrayList<>();
+        // assign start times, numbers to new segments. Add them to segments list and current list.
+        for (int i = 0; i < keyRanges.size(); i++) {
+            int number = start + i;
+            InMemorySegment segment = new InMemorySegment(number, scaleTimestamp, Long.MAX_VALUE, keyRanges.get(i).getKey(), keyRanges.get(i).getValue(), InMemorySegment.Status.Active, new ArrayList<>(), predecessors.get(i));
+            newSegments.add(segment);
+            segments.add(segment);
+            currentSegments.add(number);
+        }
+
+        return CompletableFuture.completedFuture(newSegments);
+    }
+
+    public String toString() {
+        return String.format("Current Segments:%s\nSegments:%s\n", currentSegments.toString(), segments.toString());
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStreamMetadataStore.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStreamMetadataStore.java
new file mode 100644
index 0000000..48422ac
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStreamMetadataStore.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import com.emc.pravega.stream.StreamConfiguration;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.CompletableFuture;
+
+/**
+ * In-memory stream store.
+ */
+public class InMemoryStreamMetadataStore extends AbstractStreamMetadataStore {
+
+    private final Map<String, InMemoryStream> streams = new HashMap<>();
+
+    @Override
+    public InMemoryStream getStream(String name) {
+        if (streams.containsKey(name)) {
+            return streams.get(name);
+        } else {
+            throw new StreamNotFoundException(name);
+        }
+    }
+
+    @Override
+    public CompletableFuture<Boolean> createStream(String name, StreamConfiguration configuration, long timestamp) {
+        if (!streams.containsKey(name)) {
+            InMemoryStream stream = new InMemoryStream(name);
+            stream.create(configuration, timestamp);
+            streams.put(name, stream);
+            return CompletableFuture.completedFuture(true);
+        } else {
+            throw new StreamAlreadyExistsException(name);
+        }
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStreamStore.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStreamStore.java
deleted file mode 100644
index 207affb..0000000
--- a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/InMemoryStreamStore.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package com.emc.pravega.controller.store.stream;
-
-import com.emc.pravega.stream.StreamConfiguration;
-
-import java.util.AbstractMap.SimpleEntry;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-/**
- * In-memory stream store.
- */
-public class InMemoryStreamStore implements StreamMetadataStore {
-
-    private final Map<String, Stream> streams = new HashMap<>();
-
-    private Stream getStream(String name) {
-        if (streams.containsKey(name)) {
-            Stream stream = streams.get(name);
-            return stream;
-        } else {
-            throw new StreamNotFoundException(name);
-        }
-    }
-
-    public void initialize() {
-        // TODO initialize from persistent store, create collections of appropriate size
-    }
-
-    @Override
-    public boolean createStream(String name, StreamConfiguration configuration) {
-        if (!streams.containsKey(name)) {
-            Stream stream = new Stream(name, configuration);
-            streams.put(name, stream);
-            return true;
-        } else {
-            throw new StreamAlreadyExistsException(name);
-        }
-    }
-
-    @Override
-    public boolean updateConfiguration(String name, StreamConfiguration configuration) {
-        Stream stream = getStream(name);
-        stream.setConfiguration(configuration);
-        return true;
-    }
-
-    @Override
-    public StreamConfiguration getConfiguration(String name) {
-        Stream stream = getStream(name);
-        return stream.getStreamConfiguration();
-    }
-
-    @Override
-    public Segment getSegment(String name, int number) {
-        Stream stream = getStream(name);
-        return stream.getSegment(number);
-    }
-
-    @Override
-    public SegmentFutures getActiveSegments(String name) {
-        Stream stream = getStream(name);
-        return stream.getActiveSegments();
-    }
-
-    @Override
-    public SegmentFutures getActiveSegments(String name, long timestamp) {
-        Stream stream = getStream(name);
-        return stream.getActiveSegments(timestamp);
-    }
-
-    @Override
-    public List<SegmentFutures> getNextSegments(String name, Set<Integer> completedSegments, List<SegmentFutures> currentSegments) {
-        Stream stream = getStream(name);
-        return stream.getNextSegments(completedSegments, currentSegments);
-    }
-
-    @Override
-    public List<Segment> scale(String name, List<Integer> sealedSegments, List<SimpleEntry<Double, Double>> newRanges, long scaleTimestamp) {
-        Stream stream = getStream(name);
-        return stream.scale(sealedSegments, newRanges, scaleTimestamp);
-    }
-}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/PersistentStreamBase.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/PersistentStreamBase.java
new file mode 100644
index 0000000..bbc86d4
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/PersistentStreamBase.java
@@ -0,0 +1,441 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import com.emc.pravega.common.concurrent.FutureCollectionHelper;
+import com.emc.pravega.controller.store.stream.tables.Create;
+import com.emc.pravega.controller.store.stream.tables.HistoryRecord;
+import com.emc.pravega.controller.store.stream.tables.IndexRecord;
+import com.emc.pravega.controller.store.stream.tables.Scale;
+import com.emc.pravega.controller.store.stream.tables.SegmentRecord;
+import com.emc.pravega.controller.store.stream.tables.TableHelper;
+import com.emc.pravega.stream.StreamConfiguration;
+import org.apache.commons.lang.SerializationUtils;
+import org.apache.commons.lang3.tuple.ImmutablePair;
+import org.apache.commons.lang3.tuple.ImmutableTriple;
+
+import java.util.AbstractMap;
+import java.util.List;
+import java.util.Optional;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.CompletionStage;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+public abstract class PersistentStreamBase implements Stream {
+    private final String name;
+
+    protected PersistentStreamBase(final String name) {
+        this.name = name;
+    }
+
+    @Override
+    public String getName() {
+        return this.name;
+    }
+
+    /***
+     * Creates a new stream record in the stream store.
+     * Create a new task of type Create.
+     * If create task already exists, use that and bring it to completion
+     * If no task exists, fall through all create steps. They are all idempotent
+     * <p>
+     * Create Steps:
+     * 0. Take distributed mutex
+     * 1. Create task/Fetch existing task
+     * 2. Create a new znode to store configuration
+     * 3. Create a new znode for segment table.
+     * 4. Create a new znode for history table.
+     * 5. Create a new znode for index
+     * 6. delete task
+     * 7. release mutex
+     *
+     * @param configuration stream configuration.
+     * @return : future of whether it was done or not
+     */
+    @Override
+    public CompletableFuture<Boolean> create(final StreamConfiguration configuration, long timestamp) {
+        final Create create = new Create(timestamp, configuration);
+
+        return checkStreamExists(timestamp)
+                .thenCompose(x -> createConfiguration(configuration, create))
+                .thenCompose(x -> createSegmentTable(create))
+                .thenCompose(x -> createSegmentChunk(create))
+                .thenCompose(x -> createHistoryTable(create))
+                .thenCompose(x -> createIndexTable(create))
+                .handle((ok, ex) -> {
+                    if (ex == null)
+                        return true;
+                    else {
+                        if (ex instanceof StreamAlreadyExistsException) {
+                            throw new StreamAlreadyExistsException(getName(), ex);
+                        } else
+                            throw new RuntimeException(ex);
+                    }
+                });
+    }
+
+    /**
+     * Update configuration in zk at configurationPath
+     *
+     * @param configuration new stream configuration.
+     * @return : future of boolean
+     */
+    @Override
+    public CompletableFuture<Boolean> updateConfiguration(final StreamConfiguration configuration) {
+        // replace the configurationPath zknode with new configurationPath
+        return setConfigurationData(configuration).thenApply(x -> true);
+    }
+
+    /**
+     * Fetch configuration from zk at configurationPath
+     *
+     * @return : future of stream configuration
+     */
+    @Override
+    public CompletableFuture<StreamConfiguration> getConfiguration() {
+        return getConfigurationData();
+    }
+
+    /**
+     * Compute correct znode name for the segment chunk that contains entry for this segment.
+     * Fetch the segment table chunk and retrieve the segment
+     *
+     * @param number segment number.
+     * @return : future of segment
+     */
+    @Override
+    public CompletableFuture<Segment> getSegment(final int number) {
+        return getSegmentRow(number);
+    }
+
+    /**
+     * Given segment number, find its successor candidates and then compute overlaps with its keyrange
+     * to find successors
+     *
+     * @param number segment number.
+     * @return : future of list of successor segment numbers
+     */
+    @Override
+    public CompletableFuture<List<Integer>> getSuccessors(final int number) {
+
+        final CompletableFuture[] futures = new CompletableFuture[3];
+
+        futures[0] = getSegment(number);
+        futures[1] = getIndexTable();
+        futures[2] = getHistoryTable();
+
+        return CompletableFuture.allOf(futures).thenCompose(x -> {
+
+            try {
+                final Segment segment = (Segment) futures[0].get();
+                final byte[] indexTable = (byte[]) futures[1].get();
+                final byte[] historyTable = (byte[]) futures[2].get();
+                return FutureCollectionHelper.sequence(
+                        TableHelper.findSegmentSuccessorCandidates(segment,
+                                indexTable,
+                                historyTable)
+                                .stream()
+                                .map(this::getSegment)
+                                .collect(Collectors.toList()))
+                        .thenApply(successorCandidates -> new ImmutablePair<>(segment, successorCandidates));
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        }).thenApply(x -> TableHelper.getOverlaps(x.getKey(), x.getValue()));
+    }
+
+    /**
+     * Find predecessor candidates and find overlaps with given segment's key range
+     *
+     * @param number segment number.
+     * @return : future of list of predecessor segment numbers
+     */
+    @Override
+    public CompletableFuture<List<Integer>> getPredecessors(final int number) {
+        final CompletableFuture[] futures = new CompletableFuture[3];
+
+        futures[0] = getSegment(number);
+        futures[1] = getIndexTable();
+        futures[2] = getHistoryTable();
+
+        return CompletableFuture.allOf(futures).thenCompose(x -> {
+            try {
+                final Segment segment = (Segment) futures[0].get();
+                final byte[] indexTable = (byte[]) futures[1].get();
+                final byte[] historyTable = (byte[]) futures[2].get();
+                return FutureCollectionHelper.sequence(
+                        TableHelper.findSegmentPredecessorCandidates(segment,
+                                indexTable,
+                                historyTable)
+                                .stream()
+                                .map(this::getSegment)
+                                .collect(Collectors.toList()))
+                        .thenApply(predecessorCandidates -> new ImmutablePair<>(segment, predecessorCandidates));
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        }).thenApply(x -> TableHelper.getOverlaps(x.getKey(), x.getValue()));
+    }
+
+    @Override
+    public CompletableFuture<List<Integer>> getActiveSegments() {
+        return getHistoryTable().thenApply(TableHelper::getActiveSegments);
+    }
+
+    /**
+     * if timestamp is < create time of stream, we will return empty list
+     * 1. perform binary searchIndex on index table to find timestamp
+     * 2. fetch the record from history table for the pointer in index.
+     * Note: index may be stale so we may need to fall through
+     * 3. parse the row and return the list of integers
+     *
+     * @param timestamp point in time.
+     * @return : list of active segment numbers at given time stamp
+     */
+    @Override
+    public CompletableFuture<List<Integer>> getActiveSegments(long timestamp) {
+        final CompletableFuture<byte[]> indexFuture = getIndexTable();
+
+        final CompletableFuture<byte[]> historyFuture = getHistoryTable();
+
+        return indexFuture.thenCombine(historyFuture,
+                (byte[] indexTable, byte[] historyTable) -> TableHelper.getActiveSegments(timestamp,
+                        indexTable,
+                        historyTable));
+    }
+
+    /**
+     * Scale and create are two tasks where we update the table. For scale to be legitimate, it has to be
+     * preceeded by create. Which means all appropriate tables exist.
+     * Scale Steps:
+     * 0. Take distributed mutex
+     * 1. Scale task/Fetch existing task
+     * 2. Verify if new scale input is same as existing scale task.
+     * If not, existing takes precedence. TODO: Notify caller!
+     * 3. Add new segment information in segment table.
+     * Segments could spillover into a new chunk.
+     * 4. Add entry into the history table.
+     * 5. Add entry into the index table.
+     * 6. delete task
+     * 7. release mutex
+     *
+     * @param sealedSegments segments to be sealed
+     * @param newRanges      key ranges of new segments to be created
+     * @param scaleTimestamp scaling timestamp
+     * @return : list of newly created segments
+     */
+    @Override
+    public CompletableFuture<List<Segment>> scale(final List<Integer> sealedSegments,
+                                                  final List<AbstractMap.SimpleEntry<Double, Double>> newRanges,
+                                                  final long scaleTimestamp) {
+        final Scale scale = new Scale(sealedSegments, newRanges, scaleTimestamp);
+
+        return getSegmentChunks()
+                        .thenCompose(this::getLatestChunk)
+                        .thenCompose(latestSegmentData -> addNewSegments(scale, latestSegmentData))
+                .thenCompose(startingSegmentNumber -> addHistoryRecord(sealedSegments, scale, startingSegmentNumber)
+                        .thenApply(historyOffset -> new ImmutablePair<>(startingSegmentNumber, historyOffset)))
+                .thenCompose(pair -> {
+                    final int startingSegmentNumber = pair.left;
+
+                    final int historyOffset = pair.right;
+
+                    return addIndexRecord(scale, historyOffset).thenApply(x -> startingSegmentNumber);
+                })
+                .thenCompose(startingSegmentNumber -> getSegments(newRanges.size(), startingSegmentNumber));
+    }
+
+    private CompletionStage<List<Segment>> getSegments(final int numberOfSegments,
+                                                       final int startingSegmentNumber) {
+        final List<CompletableFuture<Segment>> segments = IntStream.range(startingSegmentNumber,
+                startingSegmentNumber + numberOfSegments)
+                .boxed()
+                .map(this::getSegment)
+                .collect(Collectors.<CompletableFuture<Segment>>toList());
+        return FutureCollectionHelper.sequence(segments);
+    }
+
+    /**
+     * @param scale
+     * @param currentSegmentData
+     * @return : return starting segment number
+     */
+    private CompletableFuture<Integer> addNewSegments(
+            final Scale scale,
+            final ImmutablePair<Integer, byte[]> currentSegmentData) {
+        final int currentChunk = currentSegmentData.left;
+        final byte[] currentChunkData = currentSegmentData.right;
+
+        final int startingSegmentNumber = currentChunk * SegmentRecord.SEGMENT_CHUNK_SIZE +
+                (currentChunkData.length / SegmentRecord.SEGMENT_RECORD_SIZE);
+
+        // idempotent check
+        final Segment lastSegment = TableHelper.getSegment(startingSegmentNumber - 1, currentChunkData);
+        if (lastSegment.getStart() == scale.getScaleTimestamp()) {
+            return CompletableFuture.completedFuture(lastSegment.getNumber() - scale.getNewRanges().size() + 1);
+        }
+
+        final int maxSegmentNumberForChunk = (currentChunk + 1) * SegmentRecord.SEGMENT_CHUNK_SIZE - 1;
+
+        final int toCreate = Integer.min(maxSegmentNumberForChunk - startingSegmentNumber + 1,
+                scale.getNewRanges().size());
+
+        final byte[] updatedChunkData = TableHelper.updateSegmentTable(startingSegmentNumber,
+                currentChunkData,
+                toCreate,
+                scale.getNewRanges(),
+                scale.getScaleTimestamp()
+        );
+
+        return setSegmentTableChunk(currentChunk, updatedChunkData)
+                .thenCompose(y -> {
+                    final int chunkNumber = TableHelper.getSegmentChunkNumber(startingSegmentNumber + scale.getNewRanges().size());
+                    final int remaining = Integer.max(scale.getNewRanges().size() - toCreate, 0);
+
+                    if (remaining > 0) {
+                        byte[] newChunk = TableHelper.updateSegmentTable(chunkNumber * SegmentRecord.SEGMENT_CHUNK_SIZE,
+                                new byte[0], // new chunk
+                                remaining,
+                                scale.getNewRanges(),
+                                scale.getScaleTimestamp()
+                        );
+
+                        return createSegmentChunk(chunkNumber, newChunk);
+                    } else {
+                        return CompletableFuture.completedFuture(null);
+                    }
+                })
+                .thenApply(x -> startingSegmentNumber);
+    }
+
+    /**
+     * update history table if not already updated:
+     * fetch last record from history table.
+     * if eventTime is >= scale.scaleTimeStamp do nothing, else create record
+     *
+     * @param sealedSegments
+     * @param scale
+     * @return : future of history table offset for last entry
+     */
+    private CompletableFuture<Integer> addHistoryRecord(final List<Integer> sealedSegments,
+                                                        final Scale scale,
+                                                        final int startingSegmentNumber) {
+        return getHistoryTable()
+                .thenCompose(historyTable -> {
+                    final Optional<HistoryRecord> lastRecordOpt = HistoryRecord.readLatestRecord(historyTable);
+
+                    // scale task is not allowed unless create is done which means at least one
+                    // record in history table
+                    assert lastRecordOpt.isPresent();
+
+                    final HistoryRecord lastRecord = lastRecordOpt.get();
+
+                    // idempotent check
+                    if (lastRecord.getEventTime() >= scale.getScaleTimestamp()) {
+                        assert lastRecord.getSegments().contains(startingSegmentNumber);
+
+                        return CompletableFuture.completedFuture(lastRecord.getStartOfRowPointer());
+                    }
+
+                    final List<Integer> newActiveSegments = getNewActiveSegments(sealedSegments,
+                            scale,
+                            startingSegmentNumber,
+                            lastRecord);
+
+                    final byte[] updated = TableHelper.updateHistoryTable(historyTable,
+                            scale.getScaleTimestamp(),
+                            newActiveSegments);
+
+                    return updateHistoryTable(updated).thenApply(y -> lastRecord.getEndOfRowPointer() + 1);
+                });
+    }
+
+    private List<Integer> getNewActiveSegments(List<Integer> sealedSegments,
+                                               Scale scale,
+                                               int startingSegmentNumber,
+                                               HistoryRecord lastRecord) {
+        final List<Integer> segments = lastRecord.getSegments();
+        segments.removeAll(sealedSegments);
+        segments.addAll(
+                IntStream.range(startingSegmentNumber,
+                        startingSegmentNumber + scale.getNewRanges().size())
+                        .boxed()
+                        .collect(Collectors.toList()));
+        return segments;
+    }
+
+    private CompletionStage<Void> addIndexRecord(final Scale scale,
+                                                 final int historyOffset) {
+        return getIndexTable()
+                .thenCompose(indexTable -> {
+                    final Optional<IndexRecord> lastRecord = IndexRecord.readLatestRecord(indexTable);
+                    // check idempotent
+                    if (lastRecord.isPresent() && lastRecord.get().getEventTime() >= scale.getScaleTimestamp())
+                        return CompletableFuture.completedFuture(null);
+
+                    final byte[] updated = TableHelper.updateIndexTable(indexTable,
+                            scale.getScaleTimestamp(),
+                            historyOffset);
+                    return updateIndexTable(updated);
+                });
+    }
+
+    private CompletionStage<ImmutablePair<Integer, byte[]>> getLatestChunk(final List<String> segmentChunks) {
+        assert segmentChunks.size() > 0;
+
+        final int latestChunkNumber = segmentChunks.size() - 1;
+
+        return getSegmentTableChunk(latestChunkNumber)
+                .thenApply(segmentTableChunk -> new ImmutablePair<>(latestChunkNumber, segmentTableChunk));
+    }
+
+    abstract CompletableFuture<Boolean> checkStreamExists(long timeStamp);
+
+    abstract CompletionStage<Void> createConfiguration(StreamConfiguration configuration, Create create);
+
+    abstract CompletableFuture<Void> setConfigurationData(StreamConfiguration configuration);
+
+    abstract CompletableFuture<StreamConfiguration> getConfigurationData();
+
+    abstract CompletableFuture<Void> createSegmentTable(Create create);
+
+    abstract CompletableFuture<Void> createSegmentChunk(int chunkNumber, byte[] data);
+
+    abstract CompletableFuture<List<String>> getSegmentChunks();
+
+    abstract CompletableFuture<Segment> getSegmentRow(int number);
+
+    abstract CompletableFuture<byte[]> getSegmentTableChunk(int chunkNumber);
+
+    abstract CompletableFuture<Void> setSegmentTableChunk(int chunkNumber, byte[] data);
+
+    abstract CompletionStage<Void> createIndexTable(Create create);
+
+    abstract CompletableFuture<byte[]> getIndexTable();
+
+    abstract CompletableFuture<Void> updateIndexTable(byte[] updated);
+
+    abstract CompletionStage<Void> createHistoryTable(Create create);
+
+    abstract CompletableFuture<Void> updateHistoryTable(byte[] updated);
+
+    abstract CompletableFuture<byte[]> getHistoryTable();
+
+    abstract CompletionStage<Void> createSegmentChunk(Create create);
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Segment.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Segment.java
index a5b533e..c65206e 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Segment.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Segment.java
@@ -17,58 +17,20 @@
  */
 package com.emc.pravega.controller.store.stream;
 
-/**
- * In-memory representation of a stream segment.
- */
-import com.google.common.base.Preconditions;
 import lombok.Data;
 import lombok.ToString;
 
-import java.util.ArrayList;
-import java.util.List;
-
+/**
+ * Properties of a stream segment that don't change over its lifetime.
+ */
 @Data
 @ToString(includeFieldNames = true)
 public class Segment {
 
-    enum Status {
-        Active,
-        Sealing,
-        Sealed,
-    }
-
-    private final int number;
-    private final long start;
-    private final long end;
-    private final double keyStart;
-    private final double keyEnd;
-    private final Status status;
-    private final List<Integer> successors;
-    private final List<Integer> predecessors;
-
-    Segment(int number, long start, long end, double keyStart, double keyEnd) {
-        this.number = number;
-        this.start = start;
-        this.end = end;
-        this.keyStart = keyStart;
-        this.keyEnd = keyEnd;
-        this.status = Status.Active;
-        successors = new ArrayList<>();
-        predecessors = new ArrayList<>();
-    }
-
-    Segment(int number, long start, long end, double keyStart, double keyEnd, Status status, List<Integer> successors, List<Integer> predecessors) {
-        Preconditions.checkNotNull(successors);
-        Preconditions.checkNotNull(predecessors);
-        this.number = number;
-        this.start = start;
-        this.end = end;
-        this.keyStart = keyStart;
-        this.keyEnd = keyEnd;
-        this.status = status;
-        this.successors = successors;
-        this.predecessors = predecessors;
-    }
+    protected final int number;
+    protected final long start;
+    protected final double keyStart;
+    protected final double keyEnd;
 
     public boolean overlaps(Segment segment) {
         return segment.getKeyEnd() > keyStart && segment.getKeyStart() < keyEnd;
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StoreConfiguration.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StoreConfiguration.java
index a0053eb..0d81fd5 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StoreConfiguration.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StoreConfiguration.java
@@ -17,5 +17,9 @@
  */
 package com.emc.pravega.controller.store.stream;
 
-public interface StoreConfiguration {
+import lombok.Data;
+
+@Data
+public class StoreConfiguration {
+    private final String connectionString;
 }
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Stream.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Stream.java
index ab117a1..50bbaa5 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Stream.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/Stream.java
@@ -18,262 +18,75 @@
 package com.emc.pravega.controller.store.stream;
 
 import com.emc.pravega.stream.StreamConfiguration;
-import com.google.common.base.Preconditions;
 
-import java.util.AbstractMap.SimpleEntry;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
+import java.util.AbstractMap;
 import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.stream.Collectors;
-import java.util.stream.IntStream;
+import java.util.concurrent.CompletableFuture;
 
 /**
- * Stream properties
+ * Properties of a stream and operations that can be performed on it.
+ * Identifier for a stream is its name.
  */
-class Stream {
-    private String name;
-    private StreamConfiguration configuration;
+interface Stream {
+
+    String getName();
 
     /**
-     * Stores all segments in the stream, ordered by number, which implies that
-     * these segments are also ordered in the increaing order of their start times.
-     * Segment number is the index of that segment in this list.
+     * Create the stream, by creating/modifying underlying data structures.
+     * @param configuration stream configuration.
+     * @return boolean indicating success.
      */
-    private List<Segment> segments;
+    CompletableFuture<Boolean> create(StreamConfiguration configuration, long timeStamp);
 
     /**
-     * Stores segment numbers of currently active segments in the stream.
-     * It enables efficient access to current segments needed by producers and tailing consumers.
+     * Updates the configuration of an existing stream.
+     * @param configuration new stream configuration.
+     * @return boolean indicating whether the stream was updated.
      */
-    private List<Integer> currentSegments;
-
-    Stream(String name, StreamConfiguration configuration) {
-        this.name = name;
-        this.configuration = configuration;
-        currentSegments = new ArrayList<>();
-        segments = new ArrayList<>();
-        int numSegments = configuration.getScalingingPolicy().getMinNumSegments();
-        double keyRange = 1.0 / numSegments;
-        IntStream.range(0, numSegments)
-                .forEach(
-                        x -> {
-                            Segment segment = new Segment(x, 0, Long.MAX_VALUE, x * keyRange, (x + 1) * keyRange);
-                            segments.add(segment);
-                            currentSegments.add(x);
-                        }
-                );
-    }
-
-    String getName() {
-        return this.name;
-    }
-
-    synchronized StreamConfiguration getStreamConfiguration() {
-        return this.configuration;
-    }
-
-    synchronized void setConfiguration(StreamConfiguration configuration) {
-        this.configuration = configuration;
-    }
-
-    synchronized Segment getSegment(int number) {
-        return segments.get(number);
-    }
+    CompletableFuture<Boolean> updateConfiguration(StreamConfiguration configuration);
 
     /**
-     * Finds all successors of a given segment, that have exactly one predecessor,
-     * and hence can be included in the futures of the given segment.
-     * @param segment for which default futures are sought.
-     * @return the list of successors of specified segment who have only one predecessor.
+     * Fetches the current stream configuration.
+     * @return current stream configuration.
      */
-    private List<Integer> getDefaultFutures(Segment segment) {
-        return segment.getSuccessors().stream()
-                .filter(x -> segments.get(x).getPredecessors().size() == 1)
-                .collect(Collectors.toList());
-    }
+    CompletableFuture<StreamConfiguration> getConfiguration();
 
     /**
-     * @return the list of currently active segments
+     * Fetches details of specified segment.
+     * @param number segment number.
+     * @return segment at given number.
      */
-    synchronized SegmentFutures getActiveSegments() {
-        return new SegmentFutures(new ArrayList<>(currentSegments), Collections.emptyMap());
-    }
+    CompletableFuture<Segment> getSegment(int number);
 
     /**
-     * @return the list of segments active at a given timestamp.
-     * GetActiveSegments runs in O(n), where n is the total number of segments.
-     * It can be improved to O(k + logn), where k is the number of active segments at specified timestamp,
-     * using augmented interval tree or segment index..
-     * TODO: maintain a augmented interval tree or segment tree index
+     * @param number segment number.
+     * @return successors of specified segment.
      */
-    synchronized SegmentFutures getActiveSegments(long timestamp) {
-        List<Integer> currentSegments = new ArrayList<>();
-        Map<Integer, Integer> futureSegments = new HashMap<>();
-        int i = 0;
-        while (i < segments.size() && timestamp >= segments.get(i).getStart()) {
-            if (segments.get(i).getEnd() >= timestamp) {
-                Segment segment = segments.get(i);
-                currentSegments.add(segment.getNumber());
-                // futures is set to all the successors of segment that have this segment as the only predecessor
-                getDefaultFutures(segment).stream()
-                        .forEach(x -> futureSegments.put(x, segment.getNumber()));
-            }
-            i++;
-        }
-        return new SegmentFutures(currentSegments, futureSegments);
-    }
+    CompletableFuture<List<Integer>> getSuccessors(int number);
 
     /**
-     * @param completedSegments completely read segments.
-     * @param positions current consumer positions.
-     * @return new consumer positions including new (current or future) segments that can be read from.
+     * @param number segment number.
+     * @return predecessors of specified segment
      */
-    synchronized List<SegmentFutures> getNextSegments(Set<Integer> completedSegments, List<SegmentFutures> positions) {
-        Preconditions.checkNotNull(positions);
-        Preconditions.checkArgument(positions.size() > 0);
-
-        // successors of completed segments are interesting, which means
-        // some of them may become current, and
-        // some of them may become future
-        Set<Integer> successors = completedSegments.stream().flatMap(x -> segments.get(x).getSuccessors().stream()).collect(Collectors.toSet());
-
-        // a successor that has
-        // 1. all its predecessors completed, and
-        // 2. it is not completed yet, and
-        // 3. it is not current in any of the positions,
-        // shall become current and be added to some position
-        List<Integer> newCurrents = successors.stream().filter(x ->
-                // 1. all its predecessors completed, and
-                segments.get(x).getPredecessors().stream().allMatch(y -> completedSegments.contains(y))
-                // 2. it is not completed yet, and
-                && !completedSegments.contains(x)
-                // 3. it is not current in any of the positions
-                && positions.stream().allMatch(z -> !z.getCurrent().contains(x))
-        ).collect(Collectors.toList());
-
-        Map<Integer, List<Integer>> newFutures = new HashMap<>();
-        successors.stream().forEach(
-                x -> {
-                    // if x is not completed
-                    if (!completedSegments.contains(x)) {
-                        // number of predecessors not completed == 1
-                        List<Integer> filtered = segments.get(x).getPredecessors().stream().filter(y -> !completedSegments.contains(y)).collect(Collectors.toList());
-                        if (filtered.size() == 1) {
-                            Integer pendingPredecessor = filtered.get(0);
-                            if (newFutures.containsKey(pendingPredecessor)) {
-                                newFutures.get(pendingPredecessor).add(x);
-                            } else {
-                                List<Integer> list = new ArrayList<>();
-                                list.add(x);
-                                newFutures.put(pendingPredecessor, list);
-                            }
-                        }
-                    }
-                }
-        );
-
-        return divideSegments(newCurrents, newFutures, positions);
-    }
-
-    private List<SegmentFutures> divideSegments(List<Integer> newCurrents, Map<Integer, List<Integer>> newFutures, List<SegmentFutures> positions) {
-        List<SegmentFutures> newPositions = new ArrayList<>(positions.size());
+    CompletableFuture<List<Integer>> getPredecessors(int number);
 
-        int quotient = newCurrents.size() / positions.size();
-        int remainder = newCurrents.size() % positions.size();
-        int counter = 0;
-        for (int i = 0; i < positions.size(); i++) {
-            SegmentFutures position = positions.get(i);
+    /**
+     * @return currently active segments
+     */
+    CompletableFuture<List<Integer>> getActiveSegments();
 
-            // add the new current segments
-            List<Integer> newCurrent = new ArrayList<>(position.getCurrent());
-            int portion = (i < remainder) ? quotient + 1 : quotient;
-            for (int j = 0; j < portion; j++, counter++) {
-                newCurrent.add(newCurrents.get(counter));
-            }
-            Map<Integer, Integer> newFuture = new HashMap<>(position.getFutures());
-            // add new futures if any
-            position.getCurrent().forEach(
-                    current -> {
-                        if (newFutures.containsKey(current)) {
-                            newFutures.get(current).stream().forEach(x -> newFuture.put(x, current));
-                        }
-                    }
-            );
-            // add default futures for new and old current segments, if any
-            newCurrent.stream().forEach(
-                    x -> getDefaultFutures(segments.get(x)).stream()
-                            .forEach(
-                                    y -> {
-                                        if (!newFuture.containsKey(y)) {
-                                            newFuture.put(y, x);
-                                        }
-                                    }
-                            )
-            );
-            newPositions.add(new SegmentFutures(newCurrent, newFuture));
-        }
-        return newPositions;
-    }
+    /**
+     * @param timestamp point in time.
+     * @return the list of segments active at timestamp.
+     */
+    CompletableFuture<List<Integer>> getActiveSegments(long timestamp);
 
     /**
-     * Seals a set of segments, and adds a new set of segments as current segments.
-     * It sets appropriate endtime and successors of sealed segment.
+     * Scale the stream by sealing few segments and creating few segments
      * @param sealedSegments segments to be sealed
-     * @param keyRanges    new segments to be added as active segments
-     * @param scaleTimestamp scaling timestamp. This will be the end time of sealed segments and start time of new segments.
-     * @return the list of new segments.
+     * @param newRanges key ranges of new segments to be created
+     * @param scaleTimestamp scaling timestamp
+     * @return sequence of newly created segments
      */
-    synchronized List<Segment> scale(List<Integer> sealedSegments, List<SimpleEntry<Double, Double>> keyRanges, long scaleTimestamp) {
-        Preconditions.checkNotNull(sealedSegments);
-        Preconditions.checkNotNull(keyRanges);
-        Preconditions.checkArgument(sealedSegments.size() > 0);
-        Preconditions.checkArgument(keyRanges.size() > 0);
-
-        List<List<Integer>> predecessors = new ArrayList<>();
-        for (int i = 0; i < keyRanges.size(); i++) {
-            predecessors.add(new ArrayList<>());
-        }
-
-        int start = segments.size();
-        // assign status, end times, and successors to sealed segments.
-        // assign predecessors to new segments
-        for (Integer sealed: sealedSegments) {
-            Segment segment = segments.get(sealed);
-            List<Integer> successors = new ArrayList<>();
-
-            for (int i = 0; i < keyRanges.size(); i++) {
-                if (segment.overlaps(keyRanges.get(i).getKey(), keyRanges.get(i).getValue())) {
-                    successors.add(start + i);
-                    predecessors.get(i).add(sealed);
-                }
-            }
-            Segment sealedSegment = new Segment(sealed, segment.getStart(), scaleTimestamp, segment.getKeyStart(), segment.getKeyEnd(), Segment.Status.Sealed, successors, segment.getPredecessors());
-            segments.set(sealed, sealedSegment);
-            currentSegments.remove(sealed);
-        }
-
-        List<Segment> newSegments = new ArrayList<>();
-        // assign start times, numbers to new segments. Add them to segments list and current list.
-        for (int i = 0; i < keyRanges.size(); i++) {
-            int number = start + i;
-            Segment segment = new Segment(number, scaleTimestamp, Long.MAX_VALUE, keyRanges.get(i).getKey(), keyRanges.get(i).getValue(), Segment.Status.Active, new ArrayList<>(), predecessors.get(i));
-            newSegments.add(segment);
-            segments.add(segment);
-            currentSegments.add(number);
-        }
-
-        return newSegments;
-    }
-
-    public String toString() {
-        StringBuilder sb = new StringBuilder();
-        sb.append("Current Segments:\n");
-        sb.append(currentSegments.toString());
-        sb.append("Segments:\n");
-        sb.append(segments.toString());
-        return sb.toString();
-    }
+    CompletableFuture<List<Segment>> scale(List<Integer> sealedSegments, List<AbstractMap.SimpleEntry<Double, Double>> newRanges, long scaleTimestamp);
 }
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamMetadataStore.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamMetadataStore.java
index ac15954..a10636e 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamMetadataStore.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamMetadataStore.java
@@ -22,6 +22,7 @@ import com.emc.pravega.stream.StreamConfiguration;
 import java.util.AbstractMap.SimpleEntry;
 import java.util.List;
 import java.util.Set;
+import java.util.concurrent.CompletableFuture;
 
 /**
  * Stream Metadata
@@ -35,7 +36,7 @@ public interface StreamMetadataStore {
      * @param configuration stream configuration.
      * @return boolean indicating whether the stream was created
      */
-    boolean createStream(String name, StreamConfiguration configuration);
+    CompletableFuture<Boolean> createStream(String name, StreamConfiguration configuration, long timestamp);
 
     /**
      * Updates the configuration of an existing stream.
@@ -43,14 +44,14 @@ public interface StreamMetadataStore {
      * @param configuration new stream configuration
      * @return boolean indicating whether the stream was updated
      */
-    boolean updateConfiguration(String name, StreamConfiguration configuration);
+    CompletableFuture<Boolean> updateConfiguration(String name, StreamConfiguration configuration);
 
     /**
      * Fetches the current stream configuration.
      * @param name stream name.
      * @return current stream configuration.
      */
-    StreamConfiguration getConfiguration(String name);
+    CompletableFuture<StreamConfiguration> getConfiguration(String name);
 
     /**
      *
@@ -58,20 +59,20 @@ public interface StreamMetadataStore {
      * @param number segment number.
      * @return segment at given number.
      */
-    Segment getSegment(String name, int number);
+    CompletableFuture<Segment> getSegment(String name, int number);
 
     /**
      * @param name stream name.
      * @return currently active segments
      */
-    SegmentFutures getActiveSegments(String name);
+    CompletableFuture<List<Segment>> getActiveSegments(String name);
 
     /**
      * @param name stream name.
      * @param timestamp point in time.
      * @return the list of segments active at timestamp.
      */
-    SegmentFutures getActiveSegments(String name, long timestamp);
+    CompletableFuture<SegmentFutures> getActiveSegments(String name, long timestamp);
 
     /**
      * @param name stream name.
@@ -79,7 +80,7 @@ public interface StreamMetadataStore {
      * @param currentSegments current consumer positions.
      * @return new consumer positions including new (current or future) segments that can be read from.
      */
-    List<SegmentFutures> getNextSegments(String name, Set<Integer> completedSegments, List<SegmentFutures> currentSegments);
+    CompletableFuture<List<SegmentFutures>> getNextSegments(String name, Set<Integer> completedSegments, List<SegmentFutures> currentSegments);
 
     /**
      * Scales in or out the currently set of active segments of a stream.
@@ -90,5 +91,5 @@ public interface StreamMetadataStore {
      *                       all new segments shall have it as their start time.
      * @return the list of newly created segments
      */
-    List<Segment> scale(String name, List<Integer> sealedSegments, List<SimpleEntry<Double, Double>> newRanges, long scaleTimestamp);
+    CompletableFuture<List<Segment>> scale(String name, List<Integer> sealedSegments, List<SimpleEntry<Double, Double>> newRanges, long scaleTimestamp);
 }
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamStoreFactory.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamStoreFactory.java
index 105d9f8..8c04500 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamStoreFactory.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/StreamStoreFactory.java
@@ -31,8 +31,9 @@ public class StreamStoreFactory {
     public static StreamMetadataStore createStore(StoreType type, StoreConfiguration config) {
         switch (type) {
             case InMemory:
-                return new InMemoryStreamStore();
+                return new InMemoryStreamMetadataStore();
             case Zookeeper:
+                return new ZKStreamMetadataStore(config);
             case ECS:
             case S3:
             case HDFS:
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/ZKStream.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/ZKStream.java
new file mode 100644
index 0000000..e9989ba
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/ZKStream.java
@@ -0,0 +1,382 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import com.emc.pravega.controller.store.stream.tables.Create;
+import com.emc.pravega.controller.store.stream.tables.SegmentRecord;
+import com.emc.pravega.controller.store.stream.tables.TableHelper;
+import com.emc.pravega.controller.store.stream.tables.Utilities;
+import com.emc.pravega.stream.StreamConfiguration;
+import org.apache.commons.lang.SerializationUtils;
+import org.apache.curator.framework.CuratorFramework;
+import org.apache.curator.framework.CuratorFrameworkFactory;
+import org.apache.curator.framework.recipes.cache.NodeCache;
+import org.apache.curator.framework.recipes.cache.PathChildrenCache;
+import org.apache.curator.framework.recipes.locks.InterProcessMutex;
+import org.apache.curator.retry.ExponentialBackoffRetry;
+import org.apache.curator.utils.EnsurePath;
+import org.apache.zookeeper.data.Stat;
+
+import java.util.AbstractMap;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.CompletableFuture;
+import java.util.concurrent.CompletionStage;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+/**
+ * ZK Stream. It understands the following.
+ * 1. underlying file organization/object structure of stream metadata store.
+ * 2. how to evaluate basic read and update queries defined in the Stream interface.
+ * <p>
+ * It may cache files read from the store for its lifetime.
+ * This shall reduce store round trips for answering queries, thus making them efficient.
+ */
+class ZKStream extends PersistentStreamBase {
+    private static final String CREATION_PATH = "/streams/%s/creationPath";
+    private static final String CONFIGURATION_PATH = "/streams/%s/configurationPath";
+    private static final String SEGMENT_PATH = "/streams/%s/segmentPath";
+    private static final String HISTORY_PATH = "/streams/%s/historyPath";
+    private static final String INDEX_PATH = "/streams/%s/indexPath";
+    private static final String TASK_PATH = "/streams/%s/taskPath";
+
+    private final CuratorFramework client;
+
+    private final NodeCache configurationCache;
+    private final PathChildrenCache segmentCache;
+    private final NodeCache indexCache;
+    private final NodeCache historyCache;
+    private final NodeCache taskCache;
+
+    private final String creationPath;
+    private final String configurationPath;
+    private final String segmentPath;
+    private final String segmentChunkPathTemplate;
+    private final String historyPath;
+    private final String indexPath;
+    private final String taskPath;
+
+    public ZKStream(final String name, final String connectionString) {
+        super(name);
+
+        client = CuratorFrameworkFactory.newClient(connectionString, new ExponentialBackoffRetry(1000, 3));
+        client.start();
+        creationPath = String.format(CREATION_PATH, name);
+        configurationPath = String.format(CONFIGURATION_PATH, name);
+        segmentPath = String.format(SEGMENT_PATH, name);
+        segmentChunkPathTemplate = segmentPath + "/%s";
+        historyPath = String.format(HISTORY_PATH, name);
+        indexPath = String.format(INDEX_PATH, name);
+        taskPath = String.format(TASK_PATH, name);
+
+        segmentCache = new PathChildrenCache(client, segmentPath, true);
+        indexCache = new NodeCache(client, indexPath);
+        historyCache = new NodeCache(client, historyPath);
+        configurationCache = new NodeCache(client, configurationPath);
+        taskCache = new NodeCache(client, taskPath);
+    }
+
+    public void init() {
+        final List<CompletableFuture<Void>> futures = new ArrayList<>();
+
+        futures.add(CompletableFuture.supplyAsync(() -> start(configurationCache)));
+        futures.add(CompletableFuture.supplyAsync(() -> start(segmentCache)));
+        futures.add(CompletableFuture.supplyAsync(() -> start(historyCache)));
+        futures.add(CompletableFuture.supplyAsync(() -> start(indexCache)));
+
+        try {
+            CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()])).get();
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public void tearDown() {
+        final List<CompletableFuture<Void>> futures = new ArrayList<>();
+
+        futures.add(CompletableFuture.supplyAsync(() -> close(configurationCache)));
+        futures.add(CompletableFuture.supplyAsync(() -> close(segmentCache)));
+        futures.add(CompletableFuture.supplyAsync(() -> close(historyCache)));
+        futures.add(CompletableFuture.supplyAsync(() -> close(indexCache)));
+
+        try {
+            CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()])).get();
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        client.close();
+    }
+
+    @Override
+    CompletableFuture<Boolean> checkStreamExists(long timestamp) {
+        return checkExists(creationPath)
+                .thenCompose(exists -> {
+                    if (exists) {
+                        return getData(creationPath)
+                                .thenApply(Utilities::toLong)
+                                .thenApply(x -> x == timestamp);
+                    } else
+                        return createZNodeIfNotExist(creationPath, Utilities.toByteArray(timestamp))
+                                .thenApply(x -> true);
+                });
+    }
+
+    @Override
+    public CompletionStage<Void> createConfiguration(final StreamConfiguration configuration, final Create create) {
+        return createZNodeIfNotExist(configurationPath, SerializationUtils.serialize(configuration)).thenApply(x -> null);
+    }
+
+    @Override
+    public CompletableFuture<Void> createSegmentTable(final Create create) {
+        return createZNodeIfNotExist(segmentPath).thenApply(x -> null);
+    }
+
+    @Override
+    CompletableFuture<Void> createSegmentChunk(final int chunkNumber, final byte[] data) {
+        return createZNodeIfNotExist(String.format(segmentChunkPathTemplate, chunkNumber), data).thenApply(x -> null);
+    }
+
+    @Override
+    public CompletionStage<Void> createIndexTable(final Create create) {
+        final byte[] indexTable = TableHelper.updateIndexTable(new byte[0], create.getEventTime(), 0);
+        return createZNodeIfNotExist(indexPath, indexTable).thenApply(x -> null);
+    }
+
+    @Override
+    public CompletionStage<Void> createHistoryTable(final Create create) {
+        final int numSegments = create.getConfiguration().getScalingingPolicy().getMinNumSegments();
+        final byte[] historyTable = TableHelper.updateHistoryTable(new byte[0],
+                create.getEventTime(),
+                IntStream.range(0, numSegments).boxed().collect(Collectors.toList()));
+
+        return createZNodeIfNotExist(historyPath, historyTable).thenApply(y -> null);
+    }
+
+    @Override
+    public CompletableFuture<Void> updateHistoryTable(final byte[] updated) {
+        return setData(historyPath, updated).thenApply(x -> null);
+    }
+
+    @Override
+    public CompletionStage<Void> createSegmentChunk(final Create create) {
+        final int numSegments = create.getConfiguration().getScalingingPolicy().getMinNumSegments();
+        final int chunkFileName = 0;
+        final double keyRangeChunk = 1.0 / numSegments;
+
+        final int startingSegmentNumber = 0;
+        final List<AbstractMap.SimpleEntry<Double, Double>> newRanges = IntStream.range(0, numSegments)
+                .boxed()
+                .map(x -> new AbstractMap.SimpleEntry<>(x * keyRangeChunk, (x + 1) * keyRangeChunk))
+                .collect(Collectors.toList());
+        final int toCreate = newRanges.size();
+
+        final byte[] segmentTable = TableHelper.updateSegmentTable(startingSegmentNumber,
+                new byte[0],
+                toCreate,
+                newRanges,
+                create.getEventTime()
+        );
+
+        return createZNodeIfNotExist(String.format(segmentChunkPathTemplate, chunkFileName), segmentTable).thenApply(y -> null);
+    }
+
+    @Override
+    public CompletableFuture<Void> setConfigurationData(final StreamConfiguration configuration) {
+        return setData(configurationPath, SerializationUtils.serialize(configuration))
+                .thenApply(x -> null);
+    }
+
+    @Override
+    public CompletableFuture<StreamConfiguration> getConfigurationData() {
+        return getData(configurationCache, configurationPath)
+                .thenApply(x -> (StreamConfiguration) SerializationUtils.deserialize(x));
+    }
+
+    @Override
+    public CompletableFuture<Segment> getSegmentRow(final int number) {
+        // compute the file name based on segment number
+        final int znodeName = number / SegmentRecord.SEGMENT_CHUNK_SIZE;
+
+        return getData(segmentCache, String.format(segmentChunkPathTemplate, znodeName))
+                .thenApply(x -> TableHelper.getSegment(number, x));
+    }
+
+    @Override
+    public CompletableFuture<byte[]> getSegmentTableChunk(final int chunkNumber) {
+        return getData(segmentCache, String.format(segmentChunkPathTemplate, chunkNumber));
+    }
+
+    @Override
+    CompletableFuture<Void> setSegmentTableChunk(final int chunkNumber, final byte[] data) {
+        return setData(String.format(segmentChunkPathTemplate, chunkNumber), data).thenApply(x -> null);
+    }
+
+    @Override
+    public CompletableFuture<List<String>> getSegmentChunks() {
+        return CompletableFuture.supplyAsync(
+                () -> {
+                    try {
+                        return client.getChildren().forPath(segmentPath);
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
+                    }
+                });
+    }
+
+    @Override
+    public CompletableFuture<byte[]> getHistoryTable() {
+        return getData(historyCache, historyPath);
+    }
+
+    @Override
+    public CompletableFuture<byte[]> getIndexTable() {
+        return getData(indexCache, indexPath);
+    }
+
+    @Override
+    CompletableFuture<Void> updateIndexTable(final byte[] updated) {
+        return setData(indexPath, updated).thenApply(x -> null);
+    }
+
+    private Void start(final NodeCache cache) {
+        try {
+            cache.start();
+            return null;
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    private Void start(final PathChildrenCache cache) {
+        try {
+            cache.start();
+            return null;
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    private Void close(final NodeCache cache) {
+        try {
+            cache.close();
+            return null;
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    private Void close(final PathChildrenCache cache) {
+        try {
+            cache.close();
+            return null;
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    private CompletableFuture<Boolean> checkExists(final String path) {
+        return CompletableFuture.supplyAsync(
+                () -> {
+                    try {
+                        return client.checkExists().forPath(path);
+                    } catch (Exception e) {
+                        throw new RuntimeException(e);
+                    }
+                }).thenApply(x -> x != null);
+    }
+
+    private CompletableFuture<Void> deletePath(final String path) {
+        return CompletableFuture.supplyAsync(() -> {
+            try {
+                return client.delete().forPath(path);
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        });
+    }
+
+    private CompletableFuture<byte[]> getData(final NodeCache cache, final String path) {
+        if (cache.getCurrentData() != null)
+            return CompletableFuture.completedFuture(cache.getCurrentData().getData());
+        else return getData(path);
+    }
+
+    private CompletableFuture<byte[]> getData(final PathChildrenCache cache, final String path) {
+        if (cache.getCurrentData(path) != null)
+            return CompletableFuture.completedFuture(cache.getCurrentData(path).getData());
+        else return getData(path);
+    }
+
+    private CompletableFuture<byte[]> getData(final String path) {
+        return checkExists(path)
+                .thenApply(exists -> {
+                    if (exists) {
+                        try {
+                            return client.getData().forPath(path);
+                        } catch (Exception e) {
+                            throw new RuntimeException(e);
+                        }
+                    } else throw new RuntimeException(String.format("path %s not Found", path));
+                });
+    }
+
+    private CompletableFuture<Stat> setData(final String path, final byte[] data) {
+        return checkExists(path)
+                .thenApply(x -> {
+                    if (x) {
+                        try {
+                            return client.setData().forPath(path, data);
+                        } catch (Exception e) {
+                            throw new RuntimeException(e);
+                        }
+                    } else throw new RuntimeException(String.format("path %s not Found", path));
+                });
+    }
+
+    private CompletableFuture<String> createZNodeIfNotExist(final String path) {
+        return checkExists(path)
+                .thenApply(exists -> {
+                    if (!exists) {
+                        try {
+                            return client.create().creatingParentContainersIfNeeded().forPath(path);
+                        } catch (Exception e) {
+                            throw new RuntimeException(e);
+                        }
+                    } else return path;
+                });
+    }
+
+    private CompletableFuture<String> createZNodeIfNotExist(final String path, final byte[] data) {
+
+        return checkExists(path)
+                .thenApply(
+                        exists -> {
+                            if (!exists) {
+                                try {
+                                    return client.create().creatingParentContainersIfNeeded().forPath(path, data);
+                                } catch (Exception e) {
+                                    throw new RuntimeException(e);
+                                }
+                            } else return path;
+                        });
+    }
+
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/ZKStreamMetadataStore.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/ZKStreamMetadataStore.java
new file mode 100644
index 0000000..fab6d7b
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/ZKStreamMetadataStore.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import com.emc.pravega.stream.StreamConfiguration;
+import com.google.common.cache.CacheBuilder;
+import com.google.common.cache.CacheLoader;
+import com.google.common.cache.LoadingCache;
+import com.google.common.cache.RemovalListener;
+import com.google.common.cache.RemovalNotification;
+
+import java.util.concurrent.TimeUnit;
+
+import java.util.concurrent.CompletableFuture;
+
+/**
+ * ZK stream metadata store
+ */
+class ZKStreamMetadataStore extends AbstractStreamMetadataStore {
+
+
+    private final LoadingCache<String, ZKStream> zkStreams;
+
+    public ZKStreamMetadataStore(StoreConfiguration storeConfiguration) {
+        zkStreams = CacheBuilder.newBuilder()
+                .maximumSize(1000)
+                .expireAfterWrite(10, TimeUnit.MINUTES)
+                .removalListener(new RemovalListener<String, ZKStream>() {
+                    public void onRemoval(RemovalNotification<String, ZKStream> removal) {
+                        removal.getValue().tearDown();
+                    }
+                })
+                .build(
+                        new CacheLoader<String, ZKStream>() {
+                            public ZKStream load(String key) {
+                                ZKStream zkStream = new ZKStream(key, storeConfiguration.getConnectionString());
+                                zkStream.init();
+                                return zkStream;
+                            }
+                        });
+    }
+
+    @Override
+    ZKStream getStream(String name) {
+        return zkStreams.getUnchecked(name);
+    }
+
+    @Override
+    public CompletableFuture<Boolean> createStream(String name, StreamConfiguration configuration, long timestamp) {
+        return CompletableFuture.supplyAsync(() -> zkStreams.getUnchecked(name))
+                .thenCompose(x -> x.create(configuration, timestamp));
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Create.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Create.java
new file mode 100644
index 0000000..9c06c9a
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Create.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import com.emc.pravega.stream.StreamConfiguration;
+import lombok.Data;
+
+@Data
+public class Create implements Task<Create> {
+    private final long eventTime;
+    private final StreamConfiguration configuration;
+
+    @Override
+    public Class<Create> getType() {
+        return Create.class;
+    }
+
+    @Override
+    public Create asCreate() {
+        return this;
+    }
+
+    @Override
+    public Scale asScale() {
+        return null;
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/HistoryRecord.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/HistoryRecord.java
new file mode 100644
index 0000000..607f436
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/HistoryRecord.java
@@ -0,0 +1,138 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import lombok.Data;
+import lombok.RequiredArgsConstructor;
+import org.apache.commons.lang.ArrayUtils;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Optional;
+
+@Data
+@RequiredArgsConstructor
+/**
+ * Class corresponding to one row in the HistoryTable
+ * HistoryRecords are of variable length, so we will maintain markers for
+ * start of row and end of row. We need it in both directions because we need to traverse
+ * in both directions on the history table
+ * Row : [eventTime, List-of-active-segment-numbers]
+ */
+public class HistoryRecord {
+    private final int endOfRowPointer;
+    private final long eventTime;
+    private final List<Integer> segments;
+    private final int startOfRowPointer;
+
+    public HistoryRecord(long eventTime, List<Integer> segments, int offset) {
+        this.eventTime = eventTime;
+        this.segments = segments;
+        this.startOfRowPointer = offset;
+        endOfRowPointer = offset + (Integer.SIZE + Long.SIZE + segments.size() * Integer.SIZE + Integer.SIZE) / 8 - 1;
+    }
+
+    public static Optional<HistoryRecord> readRecord(byte[] historyTable, int offset) {
+        if (offset >= historyTable.length)
+            return Optional.empty();
+
+        int rowEndOffset = Utilities.toInt(ArrayUtils.subarray(historyTable,
+                offset,
+                offset + (Integer.SIZE / 8)));
+
+        return Optional.of(parse(ArrayUtils.subarray(historyTable,
+                offset,
+                rowEndOffset + 1)));
+    }
+
+    public static Optional<HistoryRecord> readLatestRecord(byte[] historyTable) {
+        if (historyTable.length == 0)
+            return Optional.empty();
+
+        int lastRowStartOffset = Utilities.toInt(ArrayUtils.subarray(historyTable,
+                historyTable.length - (Integer.SIZE / 8),
+                historyTable.length));
+
+        return readRecord(historyTable, lastRowStartOffset);
+    }
+
+    public static Optional<HistoryRecord> fetchNext(HistoryRecord record, byte[] historyTable) {
+        assert historyTable.length >= record.getEndOfRowPointer();
+
+        if (historyTable.length == record.getEndOfRowPointer())
+            return Optional.empty();
+        else {
+            return HistoryRecord.readRecord(historyTable, record.getEndOfRowPointer() + 1);
+        }
+    }
+
+    public static Optional<HistoryRecord> fetchPrevious(HistoryRecord record,
+                                                        byte[] historyTable) {
+        if (record.getStartOfRowPointer() == 0)
+            return Optional.empty();
+        else {
+            int rowStartOffset = Utilities.toInt(
+                    org.apache.commons.lang3.ArrayUtils.subarray(historyTable,
+                            record.getStartOfRowPointer() - (Integer.SIZE / 8),
+                            record.getStartOfRowPointer()));
+
+            return HistoryRecord.readRecord(historyTable, rowStartOffset);
+        }
+    }
+
+    private static HistoryRecord parse(byte[] b) {
+        int endOfRowPtr = Utilities.toInt(ArrayUtils.subarray(b, 0, Integer.SIZE / 8));
+        long eventTime = Utilities.toLong(ArrayUtils.subarray(b, Integer.SIZE / 8, (Integer.SIZE + Long.SIZE) / 8));
+
+        List<Integer> segments = extractSegments(ArrayUtils.subarray(b,
+                (Integer.SIZE + Long.SIZE) / 8,
+                b.length - (Integer.SIZE / 8)));
+
+        int startOfRowPtr = Utilities.toInt(ArrayUtils.subarray(b, b.length - (Integer.SIZE / 8), b.length));
+
+        return new HistoryRecord(endOfRowPtr, eventTime, segments, startOfRowPtr);
+    }
+
+    private static List<Integer> extractSegments(byte[] b) {
+        List<Integer> result = new ArrayList<>();
+        for (int i = 0; i < b.length; i = i + (Integer.SIZE / 8)) {
+            result.add(Utilities.toInt(ArrayUtils.subarray(b, i, i + (Integer.SIZE / 8))));
+        }
+        return result;
+    }
+
+    public byte[] toByteArray() {
+        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
+
+        try {
+            outputStream.write(Utilities.toByteArray(endOfRowPointer));
+            outputStream.write(Utilities.toByteArray(eventTime));
+            for (Integer segment : segments) {
+                outputStream.write(Utilities.toByteArray(segment));
+            }
+
+            outputStream.write(Utilities.toByteArray(startOfRowPointer));
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        }
+
+        return outputStream.toByteArray();
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/IndexRecord.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/IndexRecord.java
new file mode 100644
index 0000000..738f073
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/IndexRecord.java
@@ -0,0 +1,119 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import lombok.Data;
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.commons.lang3.tuple.ImmutablePair;
+import org.apache.commons.lang3.tuple.Pair;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.Optional;
+
+@Data
+/**
+ * Class corresponding to a record/row in Index table
+ * Each row is fixed size
+ * Row: [eventTime, pointer-into-history-table]
+ */
+public class IndexRecord {
+    public static final int INDEX_RECORD_SIZE = (Long.SIZE + Integer.SIZE) / 8;
+
+    private final long eventTime;
+    private final int historyOffset;
+
+    public static Optional<IndexRecord> readRecord(byte[] indexTable, int offset) {
+        if (offset >= indexTable.length)
+            return Optional.empty();
+        else
+            return Optional.of(parse(ArrayUtils.subarray(indexTable, offset, offset + INDEX_RECORD_SIZE)));
+    }
+
+    public static Optional<IndexRecord> readLatestRecord(byte[] indexTable) {
+        int lastIndexedRecordOffset = Integer.max(indexTable.length - IndexRecord.INDEX_RECORD_SIZE, 0);
+
+        return readRecord(indexTable, lastIndexedRecordOffset);
+    }
+
+    public static Optional<IndexRecord> fetchPrevious(byte[] indexTable, int offset) {
+        if (offset == 0) {
+            return Optional.<IndexRecord>empty();
+        } else {
+            return IndexRecord.readRecord(indexTable, offset - IndexRecord.INDEX_RECORD_SIZE);
+        }
+    }
+
+    public static Optional<IndexRecord> fetchNext(byte[] indexTable, int offset) {
+        if (offset + IndexRecord.INDEX_RECORD_SIZE == indexTable.length) {
+            return Optional.<IndexRecord>empty();
+        } else {
+            return readRecord(indexTable, offset + IndexRecord.INDEX_RECORD_SIZE);
+        }
+    }
+
+    public static Pair<Integer, Optional<IndexRecord>> search(long timestamp, byte[] indexTable) {
+        int lower = 0;
+        int upper = (indexTable.length - IndexRecord.INDEX_RECORD_SIZE) / IndexRecord.INDEX_RECORD_SIZE;
+        return binarySearchIndex(lower, upper, timestamp, indexTable);
+    }
+
+    private static IndexRecord parse(byte[] bytes) {
+        long eventTime = Utilities.toLong(ArrayUtils.subarray(bytes, 0, Long.SIZE / 8));
+        int offset = Utilities.toInt(ArrayUtils.subarray(bytes, Long.SIZE / 8, bytes.length));
+        return new IndexRecord(eventTime, offset);
+    }
+
+    private static Pair<Integer, Optional<IndexRecord>> binarySearchIndex(int lower,
+                                                                          int upper,
+                                                                          long timestamp,
+                                                                          byte[] indexTable) {
+        if (upper < lower || indexTable.length == 0)
+            return new ImmutablePair<>(0, Optional.empty());
+
+        int offset = ((lower + upper) / 2) * IndexRecord.INDEX_RECORD_SIZE;
+
+        IndexRecord record = IndexRecord.readRecord(indexTable, offset).get();
+
+        Optional<IndexRecord> next = IndexRecord.fetchNext(indexTable, offset);
+
+        if (record.getEventTime() <= timestamp) {
+            if (!next.isPresent() || (next.get().getEventTime() > timestamp)) {
+                return new ImmutablePair<>(offset, Optional.of(record));
+            } else {
+                return binarySearchIndex((lower + upper) / 2 + 1, upper, timestamp, indexTable);
+            }
+        } else {
+            return binarySearchIndex(lower, (lower + upper) / 2 - 1, timestamp, indexTable);
+        }
+    }
+
+    public byte[] toByteArray() {
+        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
+
+        try {
+            outputStream.write(Utilities.toByteArray(eventTime));
+            outputStream.write(Utilities.toByteArray(historyOffset));
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        }
+
+        return outputStream.toByteArray();
+    }
+
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Scale.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Scale.java
new file mode 100644
index 0000000..8638581
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Scale.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import lombok.Data;
+
+import java.util.AbstractMap;
+import java.util.List;
+
+@Data
+/**
+ * Task subclass to define scaling operations
+ * This is serialized and stored in the persistent store
+ * and used to resume partially completed scale operation
+ */
+public class Scale implements Task<Scale> {
+    private final List<Integer> sealedSegments;
+    private final List<AbstractMap.SimpleEntry<Double, Double>> newRanges;
+    private final long scaleTimestamp;
+
+    @Override
+    public Class<Scale> getType() {
+        return Scale.class;
+    }
+
+    @Override
+    public Create asCreate() {
+        return null;
+    }
+
+    @Override
+    public Scale asScale() {
+        return this;
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/SegmentRecord.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/SegmentRecord.java
new file mode 100644
index 0000000..bd78c51
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/SegmentRecord.java
@@ -0,0 +1,73 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import lombok.Data;
+import org.apache.commons.lang3.ArrayUtils;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.Optional;
+
+@Data
+/**
+ * Class represents one row/record in SegmentTable
+ * Segment table is chunked into multiple files, each containing #SEGMENT_CHUNK_SIZE records.
+ * New segment chunk-name is highest-chunk-name + 1
+ * Row: [segment-number, segment-creation-time, routing-key-floor-inclusive, routing-key-ceiling-exclusive]
+ */
+public class SegmentRecord {
+    public static final int SEGMENT_RECORD_SIZE = (Integer.SIZE + Long.SIZE + Double.SIZE + Double.SIZE) / 8;
+    public static final int SEGMENT_CHUNK_SIZE = 10;
+
+    private final int segmentNumber;
+    private final long startTime;
+    private final double routingKeyStart;
+    private final double routingKeyEnd;
+
+    public static Optional<SegmentRecord> readRecord(byte[] segmentTable, int number) {
+        int offset = (number % SegmentRecord.SEGMENT_CHUNK_SIZE) * SegmentRecord.SEGMENT_RECORD_SIZE;
+
+        if (offset >= segmentTable.length) return Optional.empty();
+        return Optional.of(parse(ArrayUtils.subarray(segmentTable, offset, offset + SEGMENT_RECORD_SIZE)));
+    }
+
+    private static SegmentRecord parse(byte[] bytes) {
+        assert bytes.length == SEGMENT_RECORD_SIZE;
+
+        return new SegmentRecord(Utilities.toInt(ArrayUtils.subarray(bytes, 0, Integer.SIZE / 8)),
+                Utilities.toLong(ArrayUtils.subarray(bytes, Integer.SIZE / 8, (Integer.SIZE + Long.SIZE) / 8)),
+                Utilities.toDouble(ArrayUtils.subarray(bytes, (Integer.SIZE + Long.SIZE) / 8, (Integer.SIZE + Long.SIZE + Double.SIZE) / 8)),
+                Utilities.toDouble(ArrayUtils.subarray(bytes, (Integer.SIZE + Long.SIZE + Double.SIZE) / 8, (Integer.SIZE + Long.SIZE + Double.SIZE + Double.SIZE) / 8)));
+    }
+
+    public byte[] toByteArray() {
+        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
+
+        try {
+            outputStream.write(Utilities.toByteArray(segmentNumber));
+            outputStream.write(Utilities.toByteArray(startTime));
+            outputStream.write(Utilities.toByteArray(routingKeyStart));
+            outputStream.write(Utilities.toByteArray(routingKeyEnd));
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        }
+
+        return outputStream.toByteArray();
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/TableHelper.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/TableHelper.java
new file mode 100644
index 0000000..a131071
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/TableHelper.java
@@ -0,0 +1,409 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import com.emc.pravega.controller.store.stream.Segment;
+
+import java.io.ByteArrayOutputStream;
+import java.util.AbstractMap;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+/**
+ * Helper class for operations pertaining to segment store tables (segment, history, index).
+ * All the processing is done locally and this class does not make any network calls.
+ * All methods are synchronous and blocking.
+ */
+public class TableHelper {
+    /**
+     * Segment Table records are of fixed size.
+     * So O(constant) operation to get segment given segmentTable Chunk.
+     *
+     * Note: this method assumes you have supplied the correct chunk
+     * @param number
+     * @param segmentTable
+     * @return
+     */
+    public static Segment getSegment(int number, byte[] segmentTable) {
+
+        Optional<SegmentRecord> recordOpt = SegmentRecord.readRecord(segmentTable, number);
+        if (recordOpt.isPresent()) {
+            SegmentRecord record = recordOpt.get();
+            return new Segment(record.getSegmentNumber(),
+                    record.getStartTime(),
+                    record.getRoutingKeyStart(),
+                    record.getRoutingKeyEnd());
+        } else
+            throw new RuntimeException(String.format("segment %d does not exist", number));
+    }
+
+    /**
+     * Helper method to determine segmentChunk information from segment number
+     * @param segmentNumber
+     * @return
+     */
+    public static int getSegmentChunkNumber(int segmentNumber) {
+        return segmentNumber / SegmentRecord.SEGMENT_CHUNK_SIZE;
+    }
+
+    public static int getNextSegmentNumber(int chunkNumber, byte[] chunkData) {
+        return chunkNumber * SegmentRecord.SEGMENT_CHUNK_SIZE +
+                (chunkData.length / SegmentRecord.SEGMENT_RECORD_SIZE);
+    }
+
+    /**
+     * Current active segments correspond to last entry in the history table.
+     * Until segment number is written to the history table it is not exposed to outside world
+     * (e.g. callers - producers and consumers)
+     * @param historyTable
+     * @return
+     */
+    public static List<Integer> getActiveSegments(byte[] historyTable) {
+        Optional<HistoryRecord> record = HistoryRecord.readLatestRecord(historyTable);
+
+        return record.isPresent() ? record.get().getSegments() : new ArrayList<>();
+    }
+
+    /**
+     * Get active segments at given timestamp
+     * Perform binary search on index table to find the record corresponding to timestamp.
+     * Note: index table may be stale or not reflect lastest state of history table.
+     * So we may need to fall through in the history table from the record being pointed to by index
+     * until we find the correct record.
+     *
+     * @param timestamp
+     * @param indexTable
+     * @param historyTable
+     * @return
+     */
+    public static List<Integer> getActiveSegments(long timestamp, byte[] indexTable, byte[] historyTable) {
+        Optional<IndexRecord> recordOpt = IndexRecord.search(timestamp, indexTable).getValue();
+        int startingOffset = recordOpt.isPresent() ? recordOpt.get().getHistoryOffset() : 0;
+
+        Optional<HistoryRecord> record = findRecordInHistoryTable(startingOffset, timestamp, historyTable);
+        return record.isPresent() ? record.get().getSegments() : new ArrayList<>();
+    }
+
+    /**
+     * Find segments from the candidate set that have overlapping key ranges with current segment
+     * @param current
+     * @param candidates
+     * @return
+     */
+    public static List<Integer> getOverlaps(
+            Segment current,
+            List<Segment> candidates) {
+        return candidates.stream().filter(x -> x.overlaps(current)).map(x -> x.getNumber()).collect(Collectors.toList());
+    }
+
+    /**
+     * Find history record from the event when the given segment was sealed.
+     * If segment is never sealed this method returns an empty list.
+     * If segment is yet to be created, this method still returns empty list.
+     *
+     * Find index that corresponds to segment start event.
+     * Perform binary search on index+history records to find segment seal event.
+     *
+     * If index table is not up to date we may have two cases:
+     * 1. Segment create time > highest event time in index
+     * 2. Segment seal time > highest event time in index
+     *
+     * For 1 we cant have any searches in index and will need to fall through
+     * History table starting from last indexed record.
+     *
+     * For 2, fall through History Table starting from last indexed record
+     * to find segment sealed event in history table.
+     *
+     * @param segment
+     * @param indexTable
+     * @param historyTable
+     * @return
+     */
+    public static List<Integer> findSegmentSuccessorCandidates(
+            Segment segment,
+            byte[] indexTable,
+            byte[] historyTable) {
+        // fetch segment start time from segment Is
+        // fetch last index Ic
+        // fetch record corresponding to Ic. If segment present in that history record, fall through history table
+        // else perform binary searchIndex
+        // Note: if segment is present at Ic, we will fall through in the history table one record at a time
+        Optional<IndexRecord> recordOpt = IndexRecord.search(segment.getStart(), indexTable)
+                .getValue();
+        int startingOffset = recordOpt.isPresent() ? recordOpt.get().getHistoryOffset() : 0;
+
+        Optional<HistoryRecord> historyRecordOpt = findRecordInHistoryTable(startingOffset,
+                segment.getStart(),
+                historyTable);
+
+        if (!historyRecordOpt.isPresent()) // segment information not in history table
+            return new ArrayList<>();
+
+        int lower = IndexRecord.search(segment.getStart(), indexTable).getKey() / IndexRecord.INDEX_RECORD_SIZE;
+
+        int upper = (indexTable.length - IndexRecord.INDEX_RECORD_SIZE) / IndexRecord.INDEX_RECORD_SIZE;
+
+        // index table may be stale, whereby we may not find segment.start to match an entry in the index table
+        Optional<IndexRecord> indexRecord = IndexRecord.readLatestRecord(indexTable);
+        // if nothing is indexed read the first record in history table, hence offset = 0
+        int lastIndexedRecordOffset = indexRecord.isPresent() ? indexRecord.get().getHistoryOffset() : 0;
+
+        Optional<HistoryRecord> lastIndexedRecord = HistoryRecord.readRecord(historyTable, lastIndexedRecordOffset);
+
+        // if segment is present in history table but its offset is greater than last indexed record,
+        // we cant do anything on index table, fall through. OR
+        // if segment exists at the last indexed record in history table, fall through,
+        // no binary search possible on index
+        if (lastIndexedRecord.get().getEventTime() < historyRecordOpt.get().getEventTime() ||
+                lastIndexedRecord.get().getSegments().contains(segment.getNumber())) {
+            // segment was sealed after the last index entry
+            HistoryRecord startPoint = lastIndexedRecord.get().getEventTime() < historyRecordOpt.get().getEventTime() ?
+                    historyRecordOpt.get() : lastIndexedRecord.get();
+            Optional<HistoryRecord> next = HistoryRecord.fetchNext(startPoint, historyTable);
+
+            while (next.isPresent() && next.get().getSegments().contains(segment.getNumber())) {
+                startPoint = next.get();
+                next = HistoryRecord.fetchNext(startPoint, historyTable);
+            }
+
+            if (next.isPresent()) {
+                return next.get().getSegments();
+            } else { // we have reached end of history table which means segment was never sealed
+                return new ArrayList<>();
+            }
+        } else {
+            // segment is definitely sealed and segment sealed event is also present in index table
+            // we should be able to find it by doing binary search on Index table
+            Optional<HistoryRecord> record = findSegmentSealedEvent(
+                    lower,
+                    upper,
+                    segment.getNumber(),
+                    indexTable,
+                    historyTable);
+
+            return record.isPresent() ? record.get().getSegments() : new ArrayList<>();
+        }
+    }
+
+    /**
+     * Method to find candidates for predecessors.
+     * If segment was created at the time of creation of stream (= no predecessors)
+     * it returns an empty list.
+     *
+     * First find the segment start time entry in the history table by using a binary
+     * search on index followed by fall through History table if index is not up to date.
+     *
+     * Fetch the record in history table that immediately preceeds segment created entry.
+     * @param segment
+     * @param indexTable
+     * @param historyTable
+     * @return
+     */
+    public static List<Integer> findSegmentPredecessorCandidates(
+            Segment segment,
+            byte[] indexTable,
+            byte[] historyTable) {
+        Optional<IndexRecord> recordOpt = IndexRecord.search(segment.getStart(), indexTable)
+                .getValue();
+        int startingOffset = recordOpt.isPresent() ? recordOpt.get().getHistoryOffset() : 0;
+
+        Optional<HistoryRecord> historyRecordOpt = findRecordInHistoryTable(startingOffset,
+                segment.getStart(),
+                historyTable);
+
+        if (!historyRecordOpt.isPresent())
+            return new ArrayList<>();
+
+        HistoryRecord record = historyRecordOpt.get();
+
+        Optional<HistoryRecord> previous = HistoryRecord.fetchPrevious(record, historyTable);
+
+        if (!previous.isPresent())
+            return new ArrayList<>();
+        else {
+            assert !previous.get().getSegments().contains(segment.getNumber());
+            return previous.get().getSegments();
+        }
+    }
+
+    /**
+     * Add new segments to the segment table
+     * This method is designed to work with chunked creation. So it takes a
+     * toCreate count and newRanges and it picks toCreate entries from the end of newranges.
+     * @param startingSegmentNumber
+     * @param segmentTable
+     * @param toCreate
+     * @param newRanges
+     * @param timeStamp
+     * @return
+     */
+    public static byte[] updateSegmentTable(int startingSegmentNumber,
+                                            byte[] segmentTable,
+                                            int toCreate,
+                                            List<AbstractMap.SimpleEntry<Double, Double>> newRanges,
+                                            long timeStamp) {
+
+        final int created = newRanges.size() - toCreate;
+
+        ByteArrayOutputStream segmentStream = new ByteArrayOutputStream();
+        try {
+            segmentStream.write(segmentTable);
+
+            IntStream.range(0, toCreate)
+                    .forEach(
+                            x -> {
+                                try {
+                                    segmentStream.write(new SegmentRecord(startingSegmentNumber + x,
+                                            timeStamp,
+                                            newRanges.get(created + x).getKey(),
+                                            newRanges.get(created + x).getValue()).toByteArray());
+                                } catch (Exception e) {
+                                    throw new RuntimeException(e);
+                                }
+                            }
+                    );
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+
+        return segmentStream.toByteArray();
+    }
+
+    /**
+     * Add a new row to the history table
+     * @param historyTable
+     * @param timestamp
+     * @param newActiveSegments
+     * @return
+     */
+    public static byte[] updateHistoryTable(byte[] historyTable,
+                                            long timestamp,
+                                            List<Integer> newActiveSegments) {
+        ByteArrayOutputStream historyStream = new ByteArrayOutputStream();
+
+        try {
+            historyStream.write(historyTable);
+            historyStream.write(new HistoryRecord(
+                    timestamp,
+                    newActiveSegments,
+                    historyTable.length)
+                    .toByteArray());
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        return historyStream.toByteArray();
+    }
+
+    /**
+     * Add a new row to index table
+     * @param indexTable
+     * @param timestamp
+     * @param historyOffset
+     * @return
+     */
+    public static byte[] updateIndexTable(byte[] indexTable,
+                                          long timestamp,
+                                          int historyOffset) {
+        ByteArrayOutputStream indexStream = new ByteArrayOutputStream();
+
+        try {
+            indexStream.write(indexTable);
+            indexStream.write(new IndexRecord(
+                    timestamp,
+                    historyOffset)
+                    .toByteArray());
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        return indexStream.toByteArray();
+    }
+
+
+    private static Optional<HistoryRecord> findRecordInHistoryTable(int startingOffset,
+                                                                    long timeStamp,
+                                                                    byte[] historyTable) {
+        Optional<HistoryRecord> recordOpt = HistoryRecord.readRecord(historyTable, startingOffset);
+
+        if (!recordOpt.isPresent() || recordOpt.get().getEventTime() > timeStamp)
+            return Optional.empty();
+
+        HistoryRecord record = recordOpt.get();
+        Optional<HistoryRecord> next = HistoryRecord.fetchNext(record, historyTable);
+
+        // check if current record is correct else we need to fall through
+        // if timestamp is > record.timestamp and less than next.timestamp
+        assert timeStamp >= record.getEventTime();
+        while (next.isPresent() && timeStamp >= next.get().getEventTime()) {
+            record = next.get();
+            next = HistoryRecord.fetchNext(record, historyTable);
+        }
+
+        return Optional.of(record);
+    }
+
+
+    private static Optional<HistoryRecord> findSegmentSealedEvent(int lower,
+                                                                  int upper,
+                                                                  int segmentNumber,
+                                                                  byte[] indexTable,
+                                                                  byte[] historyTable) {
+
+        if (lower > upper || historyTable.length == 0)
+            return Optional.empty();
+
+        int offset = ((lower + upper) / 2) * IndexRecord.INDEX_RECORD_SIZE;
+
+        Optional<IndexRecord> indexRecord = IndexRecord.readRecord(indexTable, offset);
+
+        Optional<IndexRecord> previousIndex = indexRecord.isPresent() ?
+                IndexRecord.fetchPrevious(indexTable, offset) :
+                Optional.empty();
+
+        int historyTableOffset = indexRecord.isPresent() ? indexRecord.get().getHistoryOffset() : 0;
+        Optional<HistoryRecord> record = HistoryRecord.readRecord(historyTable, historyTableOffset);
+
+        // if segment is not present in history record, check if it is present in previous
+        // if yes, we have found the segment sealed event
+        // else repeat binary searchIndex
+        if (!record.get().getSegments().contains(segmentNumber)) {
+            assert previousIndex.isPresent();
+
+            Optional<HistoryRecord> previousRecord = HistoryRecord.readRecord(historyTable,
+                    previousIndex.get().getHistoryOffset());
+            if (previousRecord.get().getSegments().contains(segmentNumber)) {
+                return record; // search complete
+            } else { // binary search lower
+                return findSegmentSealedEvent(lower,
+                        (lower + upper) / 2 - 1,
+                        segmentNumber,
+                        indexTable,
+                        historyTable);
+            }
+        } else { // binary search upper
+            // not sealed in the current location: look in second half
+            return findSegmentSealedEvent((lower + upper) / 2 + 1,
+                    upper,
+                    segmentNumber,
+                    indexTable,
+                    historyTable);
+        }
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Task.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Task.java
new file mode 100644
index 0000000..0f28c44
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Task.java
@@ -0,0 +1,34 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import java.io.Serializable;
+
+/**
+ * Base class for tasks
+ * @param <T>
+ */
+public interface Task<T extends Task> {
+    Class<T> getType();
+
+    Create asCreate();
+
+    Scale asScale();
+}
+
+
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Utilities.java b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Utilities.java
new file mode 100644
index 0000000..e618eb0
--- /dev/null
+++ b/controller/server/src/main/java/com/emc/pravega/controller/store/stream/tables/Utilities.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream.tables;
+
+import java.nio.ByteBuffer;
+
+/**
+ * Utilities class to help with conversions from numerical types to byte[] and vice versa.
+ */
+public class Utilities {
+
+    public static int toInt(byte[] b) {
+        return ByteBuffer.wrap(b).getInt();
+    }
+
+    public static long toLong(byte[] b) {
+        return ByteBuffer.wrap(b).getLong();
+    }
+
+    public static double toDouble(byte[] b) {
+        return ByteBuffer.wrap(b).getDouble();
+    }
+
+    public static byte[] toByteArray(double value) {
+        byte[] bytes = new byte[Double.SIZE / 8];
+        ByteBuffer.wrap(bytes).putDouble(value);
+        return bytes;
+    }
+
+    public static byte[] toByteArray(int value) {
+        byte[] bytes = new byte[Integer.SIZE / 8];
+        ByteBuffer.wrap(bytes).putInt(value);
+        return bytes;
+    }
+
+    public static byte[] toByteArray(long value) {
+        byte[] bytes = new byte[Long.SIZE / 8];
+        ByteBuffer.wrap(bytes).putLong(value);
+        return bytes;
+    }
+}
diff --git a/controller/server/src/main/java/com/emc/pravega/controller/util/Config.java b/controller/server/src/main/java/com/emc/pravega/controller/util/Config.java
index e629050..51d0aca 100644
--- a/controller/server/src/main/java/com/emc/pravega/controller/util/Config.java
+++ b/controller/server/src/main/java/com/emc/pravega/controller/util/Config.java
@@ -35,6 +35,7 @@ public final class Config {
     //Store configuration.
     //Stream store configuration.
     public static final String STREAM_STORE_TYPE = CONFIG.getString("config.controller.server.store.stream.type");
+    public static final String STREAM_STORE_CONNECTION_STRING = CONFIG.getString("config.controller.server.store.stream.connctionString");
 
     //HostStore configuration.
     public static final String HOST_STORE_TYPE = CONFIG.getString("config.controller.server.store.host.type");
diff --git a/controller/server/src/main/resources/application.conf b/controller/server/src/main/resources/application.conf
index 885bf70..3b6b116 100644
--- a/controller/server/src/main/resources/application.conf
+++ b/controller/server/src/main/resources/application.conf
@@ -10,6 +10,7 @@ config {
       stream {
         #Stream Store related configuration
         type = InMemory
+        connctionString = "localhost:2181"
       }
       host {
         #Host Store related configuration.
diff --git a/controller/server/src/test/java/com/emc/pravega/controller/server/v1/ConsumerApiImplTest.java b/controller/server/src/test/java/com/emc/pravega/controller/server/v1/ConsumerApiImplTest.java
deleted file mode 100644
index a2b065e..0000000
--- a/controller/server/src/test/java/com/emc/pravega/controller/server/v1/ConsumerApiImplTest.java
+++ /dev/null
@@ -1,178 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package com.emc.pravega.controller.server.v1;
-
-import static org.junit.Assert.assertEquals;
-
-import java.util.AbstractMap.SimpleEntry;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.ExecutionException;
-
-import org.apache.thrift.TException;
-import org.junit.Before;
-import org.junit.Test;
-
-import com.emc.pravega.controller.server.rpc.v1.ControllerServiceImpl;
-import com.emc.pravega.controller.store.host.Host;
-import com.emc.pravega.controller.store.host.HostControllerStore;
-import com.emc.pravega.controller.store.host.HostStoreFactory;
-import com.emc.pravega.controller.store.host.InMemoryHostControllerStoreConfig;
-import com.emc.pravega.controller.store.stream.StreamMetadataStore;
-import com.emc.pravega.controller.store.stream.StreamStoreFactory;
-import com.emc.pravega.controller.stream.api.v1.Position;
-import com.emc.pravega.controller.stream.api.v1.SegmentId;
-import com.emc.pravega.stream.ScalingPolicy;
-import com.emc.pravega.stream.StreamConfiguration;
-import com.emc.pravega.stream.impl.StreamConfigurationImpl;
-
-/**
- * ConsumerApiImpl test
- */
-public class ConsumerApiImplTest {
-
-    private static final String SCOPE = "scope";
-    private final String stream1 = "stream1";
-    private final String stream2 = "stream2";
-
-    private final StreamMetadataStore streamStore =
-            StreamStoreFactory.createStore(StreamStoreFactory.StoreType.InMemory, null);
-
-    private Map<Host, Set<Integer>> hostContainerMap = new HashMap<>();
-
-    private final HostControllerStore hostStore = HostStoreFactory.createStore(HostStoreFactory.StoreType.InMemory,
-            new InMemoryHostControllerStoreConfig(hostContainerMap));
-
-    private final ControllerServiceImpl consumer = new ControllerServiceImpl(streamStore, hostStore);
-
-    @Before
-    public void prepareStreamStore() {
-
-        final ScalingPolicy policy1 = new ScalingPolicy(ScalingPolicy.Type.FIXED_NUM_SEGMENTS, 100L, 2, 2);
-        final ScalingPolicy policy2 = new ScalingPolicy(ScalingPolicy.Type.FIXED_NUM_SEGMENTS, 100L, 2, 3);
-        final StreamConfiguration configuration1 = new StreamConfigurationImpl(SCOPE, stream1, policy1);
-        final StreamConfiguration configuration2 = new StreamConfigurationImpl(SCOPE, stream2, policy2);
-
-        // region createStream
-        streamStore.createStream(stream1, configuration1);
-        streamStore.createStream(stream2, configuration2);
-        // endregion
-
-        // region scaleSegments
-
-        SimpleEntry<Double, Double> segment1 = new SimpleEntry<>(0.5, 0.75);
-        SimpleEntry<Double, Double> segment2 = new SimpleEntry<>(0.75, 1.0);
-        streamStore.scale(stream1, Collections.singletonList(1), Arrays.asList(segment1, segment2), 20);
-
-        SimpleEntry<Double, Double> segment3 = new SimpleEntry<>(0.0, 0.5);
-        SimpleEntry<Double, Double> segment4 = new SimpleEntry<>(0.5, 0.75);
-        SimpleEntry<Double, Double> segment5 = new SimpleEntry<>(0.75, 1.0);
-        streamStore.scale(stream2, Arrays.asList(0, 1, 2), Arrays.asList(segment3, segment4, segment5), 20);
-        // endregion
-    }
-
-    @Before
-    public void prepareHostStore() {
-        Host host = new Host("localhost", 9090);
-        hostContainerMap.put(host, new HashSet<>(Collections.singletonList(0)));
-    }
-
-    @Test
-    public void testMethods() throws InterruptedException, ExecutionException, TException {
-        List<Position> positions;
-
-        positions = consumer.getPositions(SCOPE, stream1, 10, 3);
-        assertEquals(2, positions.size());
-        assertEquals(1, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(1).getOwnedSegments().size());
-        assertEquals(2, positions.get(1).getFutureOwnedSegments().size());
-
-        positions = consumer.getPositions(SCOPE, stream1, 10, 1);
-        assertEquals(1, positions.size());
-        assertEquals(2, positions.get(0).getOwnedSegments().size());
-        assertEquals(2, positions.get(0).getFutureOwnedSegments().size());
-
-        positions = consumer.getPositions(SCOPE, stream2, 10, 3);
-        assertEquals(3, positions.size());
-        assertEquals(1, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(1).getOwnedSegments().size());
-        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(2).getOwnedSegments().size());
-        assertEquals(1, positions.get(2).getFutureOwnedSegments().size());
-
-
-        Position newPosition = new Position(
-                Collections.singletonMap(new SegmentId(SCOPE, stream2, 5), 0L),
-                Collections.emptyMap());
-        positions.set(2, newPosition);
-        positions = consumer.updatePositions(SCOPE, stream2, positions);
-        assertEquals(3, positions.size());
-        assertEquals(1, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(1).getOwnedSegments().size());
-        assertEquals(1, positions.get(1).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(2).getOwnedSegments().size());
-        assertEquals(0, positions.get(2).getFutureOwnedSegments().size());
-
-        positions = consumer.getPositions(SCOPE, stream2, 10, 2);
-        assertEquals(2, positions.size());
-        assertEquals(2, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(1).getOwnedSegments().size());
-        assertEquals(1, positions.get(1).getFutureOwnedSegments().size());
-
-        positions = consumer.getPositions(SCOPE, stream1, 25, 3);
-        assertEquals(3, positions.size());
-        assertEquals(1, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(1).getOwnedSegments().size());
-        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(2).getOwnedSegments().size());
-        assertEquals(0, positions.get(2).getFutureOwnedSegments().size());
-
-        positions = consumer.getPositions(SCOPE, stream1, 25, 1);
-        assertEquals(1, positions.size());
-        assertEquals(3, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-
-        positions = consumer.getPositions(SCOPE, stream2, 25, 3);
-        assertEquals(3, positions.size());
-        assertEquals(1, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(1).getOwnedSegments().size());
-        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(2).getOwnedSegments().size());
-        assertEquals(0, positions.get(2).getFutureOwnedSegments().size());
-
-        positions = consumer.getPositions(SCOPE, stream2, 25, 2);
-        assertEquals(2, positions.size());
-        assertEquals(2, positions.get(0).getOwnedSegments().size());
-        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
-        assertEquals(1, positions.get(1).getOwnedSegments().size());
-        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
-
-    }
-}
diff --git a/controller/server/src/test/java/com/emc/pravega/controller/server/v1/ControllerServiceImplTest.java b/controller/server/src/test/java/com/emc/pravega/controller/server/v1/ControllerServiceImplTest.java
new file mode 100644
index 0000000..cef6be2
--- /dev/null
+++ b/controller/server/src/test/java/com/emc/pravega/controller/server/v1/ControllerServiceImplTest.java
@@ -0,0 +1,179 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package com.emc.pravega.controller.server.v1;
+
+import static org.junit.Assert.assertEquals;
+
+import java.util.AbstractMap.SimpleEntry;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.thrift.TException;
+import org.junit.Before;
+import org.junit.Test;
+
+import com.emc.pravega.controller.server.rpc.v1.ControllerServiceImpl;
+import com.emc.pravega.controller.store.host.Host;
+import com.emc.pravega.controller.store.host.HostControllerStore;
+import com.emc.pravega.controller.store.host.HostStoreFactory;
+import com.emc.pravega.controller.store.host.InMemoryHostControllerStoreConfig;
+import com.emc.pravega.controller.store.stream.StreamMetadataStore;
+import com.emc.pravega.controller.store.stream.StreamStoreFactory;
+import com.emc.pravega.controller.stream.api.v1.Position;
+import com.emc.pravega.controller.stream.api.v1.SegmentId;
+import com.emc.pravega.stream.ScalingPolicy;
+import com.emc.pravega.stream.StreamConfiguration;
+import com.emc.pravega.stream.impl.StreamConfigurationImpl;
+
+/**
+ * Controller service implementation test
+ */
+public class ControllerServiceImplTest {
+
+    private static final String SCOPE = "scope";
+    private final String stream1 = "stream1";
+    private final String stream2 = "stream2";
+
+    private final StreamMetadataStore streamStore =
+            StreamStoreFactory.createStore(StreamStoreFactory.StoreType.InMemory, null);
+
+    private Map<Host, Set<Integer>> hostContainerMap = new HashMap<>();
+
+    private final HostControllerStore hostStore = HostStoreFactory.createStore(HostStoreFactory.StoreType.InMemory,
+            new InMemoryHostControllerStoreConfig(hostContainerMap));
+
+    private final ControllerServiceImpl consumer = new ControllerServiceImpl(streamStore, hostStore);
+
+    @Before
+    public void prepareStreamStore() {
+
+        final ScalingPolicy policy1 = new ScalingPolicy(ScalingPolicy.Type.FIXED_NUM_SEGMENTS, 100L, 2, 2);
+        final ScalingPolicy policy2 = new ScalingPolicy(ScalingPolicy.Type.FIXED_NUM_SEGMENTS, 100L, 2, 3);
+        final StreamConfiguration configuration1 = new StreamConfigurationImpl(SCOPE, stream1, policy1);
+        final StreamConfiguration configuration2 = new StreamConfigurationImpl(SCOPE, stream2, policy2);
+
+        // region createStream
+        long timestamp = System.currentTimeMillis();
+        streamStore.createStream(stream1, configuration1, timestamp);
+        streamStore.createStream(stream2, configuration2, timestamp);
+        // endregion
+
+        // region scaleSegments
+
+        SimpleEntry<Double, Double> segment1 = new SimpleEntry<>(0.5, 0.75);
+        SimpleEntry<Double, Double> segment2 = new SimpleEntry<>(0.75, 1.0);
+        streamStore.scale(stream1, Collections.singletonList(1), Arrays.asList(segment1, segment2), 20);
+
+        SimpleEntry<Double, Double> segment3 = new SimpleEntry<>(0.0, 0.5);
+        SimpleEntry<Double, Double> segment4 = new SimpleEntry<>(0.5, 0.75);
+        SimpleEntry<Double, Double> segment5 = new SimpleEntry<>(0.75, 1.0);
+        streamStore.scale(stream2, Arrays.asList(0, 1, 2), Arrays.asList(segment3, segment4, segment5), 20);
+        // endregion
+    }
+
+    @Before
+    public void prepareHostStore() {
+        Host host = new Host("localhost", 9090);
+        hostContainerMap.put(host, new HashSet<>(Collections.singletonList(0)));
+    }
+
+    @Test
+    public void testMethods() throws InterruptedException, ExecutionException, TException {
+        List<Position> positions;
+
+        positions = consumer.getPositions(SCOPE, stream1, 10, 3).get();
+        assertEquals(2, positions.size());
+        assertEquals(1, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(1).getOwnedSegments().size());
+        assertEquals(2, positions.get(1).getFutureOwnedSegments().size());
+
+        positions = consumer.getPositions(SCOPE, stream1, 10, 1).get();
+        assertEquals(1, positions.size());
+        assertEquals(2, positions.get(0).getOwnedSegments().size());
+        assertEquals(2, positions.get(0).getFutureOwnedSegments().size());
+
+        positions = consumer.getPositions(SCOPE, stream2, 10, 3).get();
+        assertEquals(3, positions.size());
+        assertEquals(1, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(1).getOwnedSegments().size());
+        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(2).getOwnedSegments().size());
+        assertEquals(1, positions.get(2).getFutureOwnedSegments().size());
+
+
+        Position newPosition = new Position(
+                Collections.singletonMap(new SegmentId(SCOPE, stream2, 5), 0L),
+                Collections.emptyMap());
+        positions.set(2, newPosition);
+        positions = consumer.updatePositions(SCOPE, stream2, positions).get();
+        assertEquals(3, positions.size());
+        assertEquals(1, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(1).getOwnedSegments().size());
+        assertEquals(1, positions.get(1).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(2).getOwnedSegments().size());
+        assertEquals(0, positions.get(2).getFutureOwnedSegments().size());
+
+        positions = consumer.getPositions(SCOPE, stream2, 10, 2).get();
+        assertEquals(2, positions.size());
+        assertEquals(2, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(1).getOwnedSegments().size());
+        assertEquals(1, positions.get(1).getFutureOwnedSegments().size());
+
+        positions = consumer.getPositions(SCOPE, stream1, 25, 3).get();
+        assertEquals(3, positions.size());
+        assertEquals(1, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(1).getOwnedSegments().size());
+        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(2).getOwnedSegments().size());
+        assertEquals(0, positions.get(2).getFutureOwnedSegments().size());
+
+        positions = consumer.getPositions(SCOPE, stream1, 25, 1).get();
+        assertEquals(1, positions.size());
+        assertEquals(3, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+
+        positions = consumer.getPositions(SCOPE, stream2, 25, 3).get();
+        assertEquals(3, positions.size());
+        assertEquals(1, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(1).getOwnedSegments().size());
+        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(2).getOwnedSegments().size());
+        assertEquals(0, positions.get(2).getFutureOwnedSegments().size());
+
+        positions = consumer.getPositions(SCOPE, stream2, 25, 2).get();
+        assertEquals(2, positions.size());
+        assertEquals(2, positions.get(0).getOwnedSegments().size());
+        assertEquals(0, positions.get(0).getFutureOwnedSegments().size());
+        assertEquals(1, positions.get(1).getOwnedSegments().size());
+        assertEquals(0, positions.get(1).getFutureOwnedSegments().size());
+
+    }
+}
diff --git a/controller/server/src/test/java/com/emc/pravega/controller/store/stream/StreamMetadataStoreTest.java b/controller/server/src/test/java/com/emc/pravega/controller/store/stream/StreamMetadataStoreTest.java
index 8891e24..a5bbc40 100644
--- a/controller/server/src/test/java/com/emc/pravega/controller/store/stream/StreamMetadataStoreTest.java
+++ b/controller/server/src/test/java/com/emc/pravega/controller/store/stream/StreamMetadataStoreTest.java
@@ -28,6 +28,7 @@ import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
+import java.util.concurrent.ExecutionException;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
@@ -49,29 +50,28 @@ public class StreamMetadataStoreTest {
             StreamStoreFactory.createStore(StreamStoreFactory.StoreType.InMemory, null);
 
     @Test
-    public void testStreamMetadataStore() {
+    public void testStreamMetadataStore() throws InterruptedException, ExecutionException {
 
         // region createStream
-        store.createStream(stream1, configuration1);
-        store.createStream(stream2, configuration2);
+        long timestamp = System.currentTimeMillis();
+        store.createStream(stream1, configuration1, timestamp);
+        store.createStream(stream2, configuration2, timestamp);
 
-        assertEquals(stream1, store.getConfiguration(stream1).getName());
+        assertEquals(stream1, store.getConfiguration(stream1).get().getName());
         // endregion
 
         // region checkSegments
-        SegmentFutures segmentFutures = store.getActiveSegments(stream1);
-        assertEquals(2, segmentFutures.getCurrent().size());
-        assertEquals(0, segmentFutures.getFutures().size());
+        List<Segment> segments = store.getActiveSegments(stream1).get();
+        assertEquals(2, segments.size());
 
-        segmentFutures = store.getActiveSegments(stream1, 10);
+        SegmentFutures segmentFutures = store.getActiveSegments(stream1, 10).get();
         assertEquals(2, segmentFutures.getCurrent().size());
         assertEquals(0, segmentFutures.getFutures().size());
 
-        segmentFutures = store.getActiveSegments(stream2);
-        assertEquals(3, segmentFutures.getCurrent().size());
-        assertEquals(0, segmentFutures.getFutures().size());
+        segments = store.getActiveSegments(stream2).get();
+        assertEquals(3, segments.size());
 
-        segmentFutures = store.getActiveSegments(stream2, 10);
+        segmentFutures = store.getActiveSegments(stream2, 10).get();
         assertEquals(3, segmentFutures.getCurrent().size());
         assertEquals(0, segmentFutures.getFutures().size());
 
@@ -82,15 +82,14 @@ public class StreamMetadataStoreTest {
         SimpleEntry<Double, Double> segment2 = new SimpleEntry<>(0.75, 1.0);
         store.scale(stream1, Collections.singletonList(1), Arrays.asList(segment1, segment2), 20);
 
-        segmentFutures = store.getActiveSegments(stream1);
-        assertEquals(3, segmentFutures.getCurrent().size());
-        assertEquals(0, segmentFutures.getFutures().size());
+        segments = store.getActiveSegments(stream1).get();
+        assertEquals(3, segments.size());
 
-        segmentFutures = store.getActiveSegments(stream1, 30);
+        segmentFutures = store.getActiveSegments(stream1, 30).get();
         assertEquals(3, segmentFutures.getCurrent().size());
         assertEquals(0, segmentFutures.getFutures().size());
 
-        segmentFutures = store.getActiveSegments(stream1, 10);
+        segmentFutures = store.getActiveSegments(stream1, 10).get();
         assertEquals(2, segmentFutures.getCurrent().size());
         assertEquals(2, segmentFutures.getFutures().size());
 
@@ -99,11 +98,10 @@ public class StreamMetadataStoreTest {
         SimpleEntry<Double, Double> segment5 = new SimpleEntry<>(0.75, 1.0);
         store.scale(stream2, Arrays.asList(0, 1, 2), Arrays.asList(segment3, segment4, segment5), 20);
 
-        segmentFutures = store.getActiveSegments(stream1);
-        assertEquals(3, segmentFutures.getCurrent().size());
-        assertEquals(0, segmentFutures.getFutures().size());
+        segments = store.getActiveSegments(stream1).get();
+        assertEquals(3, segments.size());
 
-        segmentFutures = store.getActiveSegments(stream2, 10);
+        segmentFutures = store.getActiveSegments(stream2, 10).get();
         assertEquals(3, segmentFutures.getCurrent().size());
         assertEquals(1, segmentFutures.getFutures().size());
 
@@ -112,20 +110,20 @@ public class StreamMetadataStoreTest {
         // region getNextPosition
 
         SegmentFutures updatedPosition = new SegmentFutures(Arrays.asList(0, 5), Collections.emptyMap());
-        List<SegmentFutures> futuresList = store.getNextSegments(stream2, new HashSet<>(Arrays.asList(1, 2)), Collections.singletonList(updatedPosition));
+        List<SegmentFutures> futuresList = store.getNextSegments(stream2, new HashSet<>(Arrays.asList(1, 2)), Collections.singletonList(updatedPosition)).get();
         assertEquals(1, futuresList.size());
         assertEquals(3, futuresList.get(0).getCurrent().size());
         assertEquals(1, futuresList.get(0).getFutures().size());
         assertTrue(futuresList.get(0).getCurrent().contains(4));
 
         updatedPosition = new SegmentFutures(Arrays.asList(0, 1, 5), Collections.emptyMap());
-        futuresList = store.getNextSegments(stream2, new HashSet<>(Collections.singletonList(2)), Collections.singletonList(updatedPosition));
+        futuresList = store.getNextSegments(stream2, new HashSet<>(Collections.singletonList(2)), Collections.singletonList(updatedPosition)).get();
         assertEquals(1, futuresList.size());
         assertEquals(3, futuresList.get(0).getCurrent().size());
         assertEquals(1, futuresList.get(0).getFutures().size());
 
         updatedPosition = new SegmentFutures(Arrays.asList(0, 4, 5), Collections.emptyMap());
-        futuresList = store.getNextSegments(stream2, new HashSet<>(Collections.singletonList(1)), Collections.singletonList(updatedPosition));
+        futuresList = store.getNextSegments(stream2, new HashSet<>(Collections.singletonList(1)), Collections.singletonList(updatedPosition)).get();
         assertEquals(1, futuresList.size());
         assertEquals(3, futuresList.get(0).getCurrent().size());
         assertEquals(1, futuresList.get(0).getFutures().size());
diff --git a/controller/server/src/test/java/com/emc/pravega/controller/store/stream/TableHelperTest.java b/controller/server/src/test/java/com/emc/pravega/controller/store/stream/TableHelperTest.java
new file mode 100644
index 0000000..5d1de2f
--- /dev/null
+++ b/controller/server/src/test/java/com/emc/pravega/controller/store/stream/TableHelperTest.java
@@ -0,0 +1,396 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import static org.junit.Assert.assertEquals;
+
+import com.emc.pravega.controller.store.stream.tables.SegmentRecord;
+import com.emc.pravega.controller.store.stream.tables.TableHelper;
+import com.google.common.collect.Lists;
+import org.junit.Test;
+
+import java.util.AbstractMap;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+public class TableHelperTest {
+    @Test
+    public void getSegmentTest() {
+        long time = System.currentTimeMillis();
+        byte[] segmentTable = createSegmentTable(5, time);
+        assertEquals(segmentTable.length / SegmentRecord.SEGMENT_RECORD_SIZE, 5);
+
+        Segment segment = TableHelper.getSegment(0, segmentTable);
+        assertEquals(segment.getNumber(), 0);
+        assertEquals(segment.getStart(), time);
+        assertEquals(segment.getKeyStart(), 0, 0);
+        assertEquals(segment.getKeyEnd(), 1.0 / 5, 0);
+
+        time = System.currentTimeMillis();
+        segmentTable = updateSegmentTable(segmentTable, 5, time);
+        assertEquals(segmentTable.length / SegmentRecord.SEGMENT_RECORD_SIZE, 10);
+
+        segment = TableHelper.getSegment(9, segmentTable);
+        assertEquals(segment.getNumber(), 9);
+        assertEquals(segment.getStart(), time);
+        assertEquals(segment.getKeyStart(), 1.0 / 5 * 4, 0);
+        assertEquals(segment.getKeyEnd(), 1.0, 0);
+    }
+
+    @Test
+    public void getActiveSegmentsTest() {
+        final List<Integer> newSegments = Lists.newArrayList(0, 1, 2, 3, 4);
+        long timestamp = System.currentTimeMillis();
+        byte[] historyTable = TableHelper.updateHistoryTable(new byte[0], timestamp, newSegments);
+        List<Integer> activeSegments = TableHelper.getActiveSegments(historyTable);
+        assertEquals(activeSegments, newSegments);
+
+        List<Integer> newSegments2 = Lists.newArrayList(5, 6, 7, 8, 9);
+
+        historyTable = TableHelper.updateHistoryTable(historyTable, System.currentTimeMillis() + 1, newSegments2);
+        activeSegments = TableHelper.getActiveSegments(historyTable);
+        assertEquals(activeSegments, newSegments2);
+
+        activeSegments = TableHelper.getActiveSegments(timestamp, new byte[0], historyTable);
+        assertEquals(newSegments, activeSegments);
+    }
+
+    private Segment getSegment(int number, List<Segment> segments) {
+        return segments.stream().filter(x -> x.getNumber() == number).findAny().get();
+    }
+
+    @Test
+    public void predecessorAndSuccessorTest() {
+        // multiple rows in history table, find predecessor
+        // - more than one predecessor
+        // - one predecessor
+        // - no predecessor
+        // - immediate predecessor
+        // - predecessor few rows behind
+        List<Segment> segments = new ArrayList<>();
+        List<Integer> newSegments = Lists.newArrayList(0, 1, 2, 3, 4);
+        long timestamp = System.currentTimeMillis();
+        Segment zero = new Segment(0, timestamp, 0, 0.2);
+        segments.add(zero);
+        Segment one = new Segment(1, timestamp, 0.2, 0.4);
+        segments.add(one);
+        Segment two = new Segment(2, timestamp, 0.4, 0.6);
+        segments.add(two);
+        Segment three = new Segment(3, timestamp, 0.6, 0.8);
+        segments.add(three);
+        Segment four = new Segment(4, timestamp, 0.8, 1);
+        segments.add(four);
+
+        List<Integer> predecessors, successors;
+
+        // find predecessors and successors when update to history and index table hasnt happened
+        predecessors = TableHelper.getOverlaps(zero,
+                TableHelper
+                        .findSegmentPredecessorCandidates(zero,
+                                new byte[0],
+                                new byte[0])
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(zero,
+                TableHelper.findSegmentSuccessorCandidates(zero,
+                        new byte[0],
+                        new byte[0])
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+
+        assertEquals(predecessors, new ArrayList<Integer>());
+        assertEquals(successors, new ArrayList<Integer>());
+
+        byte[] historyTable = TableHelper.updateHistoryTable(new byte[0], timestamp, newSegments);
+        byte[] indexTable = TableHelper.updateIndexTable(new byte[0], timestamp, 0);
+
+        int nextHistoryOffset = historyTable.length;
+
+        // 3, 4 -> 5
+        newSegments = Lists.newArrayList(0, 1, 2, 5);
+        timestamp = System.currentTimeMillis() + 1;
+        Segment five = new Segment(5, timestamp, 0.6, 1);
+        segments.add(five);
+
+        historyTable = TableHelper.updateHistoryTable(historyTable, timestamp, newSegments);
+        indexTable = TableHelper.updateIndexTable(indexTable, timestamp, nextHistoryOffset);
+        nextHistoryOffset = historyTable.length;
+
+        // 1 -> 6,7.. 2,5 -> 8
+        newSegments = Lists.newArrayList(0, 6, 7, 8);
+        timestamp = System.currentTimeMillis() + 2;
+        Segment six = new Segment(6, timestamp, 0.2, 0.3);
+        segments.add(six);
+        Segment seven = new Segment(7, timestamp, 0.3, 0.4);
+        segments.add(seven);
+        Segment eight = new Segment(8, timestamp, 0.4, 1);
+        segments.add(eight);
+
+        historyTable = TableHelper.updateHistoryTable(historyTable, timestamp, newSegments);
+        indexTable = TableHelper.updateIndexTable(indexTable, timestamp, nextHistoryOffset);
+        nextHistoryOffset = historyTable.length;
+
+        // 7 -> 9,10.. 8 -> 10, 11
+        newSegments = Lists.newArrayList(0, 6, 9, 10, 11);
+        timestamp = System.currentTimeMillis() + 3;
+        Segment nine = new Segment(9, timestamp, 0.3, 0.35);
+        segments.add(nine);
+        Segment ten = new Segment(10, timestamp, 0.35, 0.6);
+        segments.add(ten);
+        Segment eleven = new Segment(11, timestamp, 0.6, 1);
+        segments.add(eleven);
+
+        historyTable = TableHelper.updateHistoryTable(historyTable, timestamp, newSegments);
+        // find predecessor and successor with index table being stale
+        predecessors = TableHelper.getOverlaps(ten,
+                TableHelper.findSegmentPredecessorCandidates(ten,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(seven,
+                TableHelper.findSegmentSuccessorCandidates(seven,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+
+        assertEquals(predecessors, Lists.newArrayList(7, 8));
+        assertEquals(successors, Lists.newArrayList(9, 10));
+
+        indexTable = TableHelper.updateIndexTable(indexTable, timestamp, nextHistoryOffset);
+
+        // 0 has no successor and no predecessor
+        // 10 has multiple predecessor
+        // 1 has a successor few rows down
+
+        predecessors = TableHelper.getOverlaps(zero,
+                TableHelper.findSegmentPredecessorCandidates(zero,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(zero,
+                TableHelper.findSegmentSuccessorCandidates(zero,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+
+        assertEquals(predecessors, new ArrayList<Integer>());
+        assertEquals(successors, new ArrayList<Integer>());
+
+        predecessors = TableHelper.getOverlaps(one,
+                TableHelper.findSegmentPredecessorCandidates(one,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(one,
+                TableHelper.findSegmentSuccessorCandidates(one,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments))
+                        .collect(Collectors.toList()));
+        assertEquals(predecessors, new ArrayList<Integer>());
+        assertEquals(successors, Lists.newArrayList(6, 7));
+
+        predecessors = TableHelper.getOverlaps(two,
+                TableHelper.findSegmentPredecessorCandidates(two,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(two,
+                TableHelper.findSegmentSuccessorCandidates(two,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, new ArrayList<Integer>());
+        assertEquals(successors, Lists.newArrayList(8));
+
+        predecessors = TableHelper.getOverlaps(three,
+                TableHelper.findSegmentPredecessorCandidates(three,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(three,
+                TableHelper.findSegmentSuccessorCandidates(three,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, new ArrayList<Integer>());
+        assertEquals(successors, Lists.newArrayList(5));
+
+        predecessors = TableHelper.getOverlaps(four,
+                TableHelper.findSegmentPredecessorCandidates(four,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(four,
+                TableHelper.findSegmentSuccessorCandidates(four,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, new ArrayList<Integer>());
+        assertEquals(successors, Lists.newArrayList(5));
+
+        predecessors = TableHelper.getOverlaps(five,
+                TableHelper.findSegmentPredecessorCandidates(five,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(five,
+                TableHelper.findSegmentSuccessorCandidates(five,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, Lists.newArrayList(3, 4));
+        assertEquals(successors, Lists.newArrayList(8));
+
+        predecessors = TableHelper.getOverlaps(six,
+                TableHelper.findSegmentPredecessorCandidates(six,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(six,
+                TableHelper.findSegmentSuccessorCandidates(six,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, Lists.newArrayList(1));
+        assertEquals(successors, new ArrayList());
+
+        predecessors = TableHelper.getOverlaps(seven,
+                TableHelper.findSegmentPredecessorCandidates(seven,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(seven,
+                TableHelper.findSegmentSuccessorCandidates(seven,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, Lists.newArrayList(1));
+        assertEquals(successors, Lists.newArrayList(9, 10));
+
+        predecessors = TableHelper.getOverlaps(eight,
+                TableHelper.findSegmentPredecessorCandidates(eight,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(eight,
+                TableHelper.findSegmentSuccessorCandidates(eight,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, Lists.newArrayList(2, 5));
+        assertEquals(successors, Lists.newArrayList(10, 11));
+
+        predecessors = TableHelper.getOverlaps(nine,
+                TableHelper.findSegmentPredecessorCandidates(nine,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(nine,
+                TableHelper.findSegmentSuccessorCandidates(nine,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, Lists.newArrayList(7));
+        assertEquals(successors, new ArrayList());
+
+        predecessors = TableHelper.getOverlaps(ten,
+                TableHelper.findSegmentPredecessorCandidates(ten,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(ten,
+                TableHelper.findSegmentSuccessorCandidates(ten,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, Lists.newArrayList(7, 8));
+        assertEquals(successors, new ArrayList());
+
+        predecessors = TableHelper.getOverlaps(eleven,
+                TableHelper.findSegmentPredecessorCandidates(eleven,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        successors = TableHelper.getOverlaps(eleven,
+                TableHelper.findSegmentSuccessorCandidates(eleven,
+                        indexTable,
+                        historyTable)
+                        .stream()
+                        .map(x -> getSegment(x, segments)).collect(Collectors.toList()));
+        assertEquals(predecessors, Lists.newArrayList(8));
+        assertEquals(successors, new ArrayList());
+    }
+
+    private byte[] createSegmentTable(int numSegments, long eventTime) {
+        final double keyRangeChunk = 1.0 / numSegments;
+
+        List<AbstractMap.SimpleEntry<Double, Double>> newRanges = IntStream.range(0, numSegments)
+                .boxed()
+                .map(x -> new AbstractMap.SimpleEntry<>(x * keyRangeChunk, (x + 1) * keyRangeChunk))
+                .collect(Collectors.toList());
+
+        return TableHelper.updateSegmentTable(0, new byte[0], numSegments, newRanges, eventTime);
+    }
+
+    private byte[] updateSegmentTable(byte[] segmentTable, int numSegments, long eventTime) {
+        final double keyRangeChunk = 1.0 / numSegments;
+        final int startingSegNum = segmentTable.length / SegmentRecord.SEGMENT_RECORD_SIZE;
+        List<AbstractMap.SimpleEntry<Double, Double>> newRanges = IntStream.range(0, numSegments)
+                .boxed()
+                .map(x -> new AbstractMap.SimpleEntry<>(x * keyRangeChunk, (x + 1) * keyRangeChunk))
+                .collect(Collectors.toList());
+
+        return TableHelper.updateSegmentTable(startingSegNum, segmentTable, numSegments, newRanges, eventTime);
+    }
+}
+
diff --git a/controller/server/src/test/java/com/emc/pravega/controller/store/stream/ZkStreamTest.java b/controller/server/src/test/java/com/emc/pravega/controller/store/stream/ZkStreamTest.java
new file mode 100644
index 0000000..c89f366
--- /dev/null
+++ b/controller/server/src/test/java/com/emc/pravega/controller/store/stream/ZkStreamTest.java
@@ -0,0 +1,196 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package com.emc.pravega.controller.store.stream;
+
+import com.emc.pravega.controller.store.stream.tables.SegmentRecord;
+import com.emc.pravega.stream.ScalingPolicy;
+import com.emc.pravega.stream.impl.StreamConfigurationImpl;
+import com.google.common.collect.Lists;
+import org.apache.curator.framework.CuratorFramework;
+import org.apache.curator.framework.CuratorFrameworkFactory;
+import org.apache.curator.retry.RetryOneTime;
+import org.apache.curator.test.TestingServer;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Ignore;
+import org.junit.Test;
+
+import java.util.AbstractMap;
+import java.util.List;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+public class ZkStreamTest {
+
+    private TestingServer zkTestServer;
+    private CuratorFramework cli;
+
+    @Before
+    public void startZookeeper() throws Exception {
+        zkTestServer = new TestingServer(2181);
+        cli = CuratorFrameworkFactory.newClient(zkTestServer.getConnectString(), new RetryOneTime(2000));
+    }
+
+    @After
+    public void stopZookeeper() throws Exception {
+        cli.close();
+        zkTestServer.stop();
+    }
+
+
+    @Test
+    public void TestZkStream() throws Exception {
+        final ScalingPolicy policy = new ScalingPolicy(ScalingPolicy.Type.FIXED_NUM_SEGMENTS, 100L, 2, 5);
+
+        final StoreConfiguration config = new StoreConfiguration(zkTestServer.getConnectString());
+        final StreamMetadataStore store = StreamStoreFactory.createStore(StreamStoreFactory.StoreType.Zookeeper, config);
+        final String streamName = "test";
+
+        StreamConfigurationImpl streamConfig = new StreamConfigurationImpl(streamName, streamName, policy);
+        long start = System.currentTimeMillis();
+        store.createStream(streamName, streamConfig, start).get();
+
+        List<Segment> segments = store.getActiveSegments(streamName).get();
+        assertEquals(segments.size(), 5);
+        assertTrue(segments.stream().allMatch(x -> Lists.newArrayList(0, 1, 2, 3, 4).contains(x.getNumber())));
+
+
+        assertEquals(store.getConfiguration(streamName).get(), streamConfig);
+
+        List<AbstractMap.SimpleEntry<Double, Double>> newRanges;
+
+        // existing range 0 = 0 - .2, 1 = .2 - .4, 2 = .4 - .6, 3 = .6 - .8, 4 = .8 - 1.0
+
+        // 3, 4 -> 5 = .6 - 1.0
+        newRanges = Lists.newArrayList(
+                new AbstractMap.SimpleEntry<Double, Double>(0.6, 1.0));
+
+        long scale1 = start + 10;
+        store.scale(streamName, Lists.newArrayList(3, 4), newRanges, scale1).get();
+
+        segments = store.getActiveSegments(streamName).get();
+        assertEquals(segments.size(), 4);
+        assertTrue(segments.stream().allMatch(x -> Lists.newArrayList(0, 1, 2, 5).contains(x.getNumber())));
+
+        // 1 -> 6 = 0.2 -.3, 7 = .3 - .4
+        // 2,5 -> 8 = .4 - 1.0
+        newRanges = Lists.newArrayList(
+                new AbstractMap.SimpleEntry<Double, Double>(0.2, 0.3),
+                new AbstractMap.SimpleEntry<Double, Double>(0.3, 0.4),
+                new AbstractMap.SimpleEntry<Double, Double>(0.4, 1.0));
+
+        long scale2 = scale1 + 10;
+        store.scale(streamName, Lists.newArrayList(1, 2, 5), newRanges, scale2).get();
+
+        segments = store.getActiveSegments(streamName).get();
+        assertEquals(segments.size(), 4);
+        assertTrue(segments.stream().allMatch(x -> Lists.newArrayList(0, 6, 7, 8).contains(x.getNumber())));
+
+        // 7 -> 9 = .3 - .35, 10 = .35 - .6
+        // 8 -> 10 = .35 - .6, 11 = .6 - 1.0
+        newRanges = Lists.newArrayList(
+                new AbstractMap.SimpleEntry<Double, Double>(0.3, 0.35),
+                new AbstractMap.SimpleEntry<Double, Double>(0.35, 0.6),
+                new AbstractMap.SimpleEntry<Double, Double>(0.6, 1.0));
+
+        long scale3 = scale2 + 10;
+        store.scale(streamName, Lists.newArrayList(7, 8), newRanges, scale3).get();
+
+        segments = store.getActiveSegments(streamName).get();
+        assertEquals(segments.size(), 5);
+        assertTrue(segments.stream().allMatch(x -> Lists.newArrayList(0, 6, 9, 10, 11).contains(x.getNumber())));
+
+        // start -1
+        SegmentFutures segmentFutures = store.getActiveSegments(streamName, start - 1).get();
+        assertEquals(segmentFutures.getCurrent().size(), 0);
+
+        // start + 1
+        segmentFutures = store.getActiveSegments(streamName, start + 1).get();
+        assertEquals(segmentFutures.getCurrent().size(), 5);
+        assertTrue(segmentFutures.getCurrent().containsAll(Lists.newArrayList(0, 1, 2, 3, 4)));
+
+        // scale1 + 1
+        segmentFutures = store.getActiveSegments(streamName, scale1 + 1).get();
+        assertEquals(segmentFutures.getCurrent().size(), 4);
+        assertTrue(segmentFutures.getCurrent().containsAll(Lists.newArrayList(0, 1, 2, 5)));
+
+        // scale2 + 1
+        segmentFutures = store.getActiveSegments(streamName, scale2 + 1).get();
+        assertEquals(segmentFutures.getCurrent().size(), 4);
+        assertTrue(segmentFutures.getCurrent().containsAll(Lists.newArrayList(0, 6, 7, 8)));
+
+        // scale3 + 1
+        segmentFutures = store.getActiveSegments(streamName, scale3 + 1).get();
+        assertEquals(segmentFutures.getCurrent().size(), 5);
+        assertTrue(segmentFutures.getCurrent().containsAll(Lists.newArrayList(0, 6, 9, 10, 11)));
+
+        // scale 3 + 100
+        segmentFutures = store.getActiveSegments(streamName, scale3 + 100).get();
+        assertEquals(segmentFutures.getCurrent().size(), 5);
+        assertTrue(segmentFutures.getCurrent().containsAll(Lists.newArrayList(0, 6, 9, 10, 11)));
+    }
+
+    @Ignore("run manually")
+    @Test
+    public void TestZkStreamChunking() throws Exception {
+        final ScalingPolicy policy = new ScalingPolicy(ScalingPolicy.Type.FIXED_NUM_SEGMENTS, 100L, 2, 6);
+
+        final StoreConfiguration config = new StoreConfiguration(zkTestServer.getConnectString());
+        final StreamMetadataStore store = StreamStoreFactory.createStore(StreamStoreFactory.StoreType.Zookeeper, config);
+        final String streamName = "test2";
+
+        StreamConfigurationImpl streamConfig = new StreamConfigurationImpl(streamName, streamName, policy);
+        long start = System.currentTimeMillis();
+        store.createStream(streamName, streamConfig, start).get();
+
+        List<Segment> initial = store.getActiveSegments(streamName).get();
+        assertEquals(initial.size(), 6);
+        assertTrue(initial.stream().allMatch(x -> Lists.newArrayList(0, 1, 2, 3, 4, 5).contains(x.getNumber())));
+
+
+        assertEquals(store.getConfiguration(streamName).get(), streamConfig);
+
+
+        IntStream.range(0, SegmentRecord.SEGMENT_CHUNK_SIZE + 2).forEach(x -> {
+            List<AbstractMap.SimpleEntry<Double, Double>> newRanges = Lists.newArrayList(
+                    new AbstractMap.SimpleEntry<Double, Double>(0.0, 0.2),
+                    new AbstractMap.SimpleEntry<Double, Double>(0.2, 0.4),
+                    new AbstractMap.SimpleEntry<Double, Double>(0.4, 0.6),
+                    new AbstractMap.SimpleEntry<Double, Double>(0.6, 0.8),
+                    new AbstractMap.SimpleEntry<Double, Double>(0.8, 0.9),
+                    new AbstractMap.SimpleEntry<Double, Double>(0.9, 1.0));
+
+            long scaleTs = start + 10 * (x + 1);
+
+            try {
+
+                List<Integer> list = IntStream.range(x * 6, (x + 1) * 6).boxed().collect(Collectors.toList());
+                store.scale(streamName, list, newRanges, scaleTs).get();
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        });
+
+        List<Segment> segments = store.getActiveSegments(streamName).get();
+        assertEquals(segments.size(), 6);
+
+    }
+}
diff --git a/integrationtests/src/main/java/com/emc/pravega/demo/StartProducer.java b/integrationtests/src/main/java/com/emc/pravega/demo/StartProducer.java
index 66deb2a..de22c98 100644
--- a/integrationtests/src/main/java/com/emc/pravega/demo/StartProducer.java
+++ b/integrationtests/src/main/java/com/emc/pravega/demo/StartProducer.java
@@ -21,7 +21,9 @@ import java.net.URI;
 
 import com.emc.pravega.stream.Producer;
 import com.emc.pravega.stream.ProducerConfig;
+import com.emc.pravega.stream.ScalingPolicy;
 import com.emc.pravega.stream.Stream;
+import com.emc.pravega.stream.impl.StreamConfigurationImpl;
 import com.emc.pravega.stream.impl.JavaSerializer;
 import com.emc.pravega.stream.impl.StreamManagerImpl;
 
@@ -35,11 +37,12 @@ public class StartProducer {
         String scope = "Scope1";
         String streamName = "Stream1";
         String testString = "Hello world: ";
-        URI controllerUri = new URI(endpoint + ":" + port);
+        URI controllerUri = new URI("tcp://" + endpoint + ":" + port);
 
         @Cleanup
         StreamManagerImpl streamManager = new StreamManagerImpl(scope, controllerUri);
-        Stream stream = streamManager.createStream(streamName, null);
+        ScalingPolicy policy = new ScalingPolicy(ScalingPolicy.Type.FIXED_NUM_SEGMENTS, 100L, 2, 2);
+        Stream stream = streamManager.createStream(streamName, new StreamConfigurationImpl(scope, streamName, policy));
         // TODO: remove sleep. It ensures pravega host handles createsegment call from controller before we publish.
         Thread.sleep(1000);
 
